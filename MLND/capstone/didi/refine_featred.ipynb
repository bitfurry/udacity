{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOUT THIS NOTEBOOK\n",
    "## Purpose\n",
    "This notebook attempts to improve algorithm performance by reducing the feature set (overfitting).\n",
    "## Input\n",
    "'data_set.pickle' generated by 'data_processing.ipynb'.\n",
    "## Output\n",
    "Results of model fitting: plots, parameters and scores.\n",
    "## Tasks Performed\n",
    "* Load library packages\n",
    "* Load pickle file\n",
    "* Split data into train & test sets\n",
    "    * Train: weeks 1 & 2, Test: week 3\n",
    "    * Perform feature scaling\n",
    "* Implement functions for common tasks\n",
    "    * Scoring\n",
    "    * Learning curves\n",
    "* Run the following algorithms:\n",
    "    * Random Forest Regressor\n",
    "    * Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD LIBRARY PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in packages from os, numpy, pandas, matplotlib, seaborn, sklearn & six\n"
     ]
    }
   ],
   "source": [
    "# Import the required library packages\n",
    "import os\n",
    "import re\n",
    "import timeit\n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.learning_curve import validation_curve\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Settings for matplotlib, Seaborn\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Set font sizes for matplots\n",
    "plt.rcParams.update({'font.size': 15, \n",
    "                     'legend.fontsize': 'medium', \n",
    "                     'axes.titlesize': 'medium', \n",
    "                     'axes.labelsize': 'medium'})\n",
    "\n",
    "print 'Read in packages from os, numpy, pandas, matplotlib, seaborn, sklearn & six'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD PICKLE FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ptrain_set (199584, 55)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'data_set.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    pdata_set = save['data_set']\n",
    "    del save\n",
    "    print 'Loaded ptrain_set', pdata_set.shape\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['district_id', 'num_day', 'time_slot', 'week_day', 'demand',\n",
       "       'demand_t-1', 'demand_t-2', 'demand_t-3', 'supply', 'supply_t-1',\n",
       "       'supply_t-2', 'supply_t-3', 'gap', 'weather', 'temperature',\n",
       "       'pollution', 'poi_pc1', 'poi_pc2', 'poi_pc3', 'poi_pc4',\n",
       "       'poi_cluster', 'tj_lvl1', 'tj_lvl2', 'tj_lvl3', 'tj_lvl4', 'dist_0',\n",
       "       'dist_1', 'dist_2', 'dist_3', 'dist_4', 'dist_5', 'dist_6',\n",
       "       'numday_0', 'numday_1', 'numday_2', 'numday_3', 'numday_4', 'ts_0',\n",
       "       'ts_1', 'ts_2', 'ts_3', 'ts_4', 'ts_5', 'ts_6', 'ts_7', 'weekday_0',\n",
       "       'weekday_1', 'weekday_2', 'poi_0', 'poi_1', 'poi_2', 'wthr_0',\n",
       "       'wthr_1', 'wthr_2', 'wthr_3'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdata_set.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD GAP FOR PREVIOUS TIME SLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new gap features for previous 3 time slots\n",
    "pdata_set['gap_t-1'] = (pdata_set['demand_t-1'] - pdata_set['supply_t-1'])\n",
    "pdata_set['gap_t-2'] = (pdata_set['demand_t-2'] - pdata_set['supply_t-2'])\n",
    "pdata_set['gap_t-3'] = (pdata_set['demand_t-3'] - pdata_set['supply_t-3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT DATA INTO TRAIN & TEST SETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use weeks 1 & 2 for training, week 3 for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train, X_test: (133056, 58) (66528, 58) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_days     = range(1,15)\n",
    "test_days      = range(15, 22)\n",
    "\n",
    "X_train     = pdata_set[(pdata_set['num_day'].isin(train_days))]\n",
    "X_test      = pdata_set[(pdata_set['num_day'].isin(test_days))]\n",
    "\n",
    "print \"Shape of X_train, X_test:\", X_train.shape, X_test.shape, \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate scaled features for train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "gap_predictors = ['demand_t-1', 'demand_t-2', 'demand_t-3',\n",
    "                  'supply_t-1', 'supply_t-2', 'supply_t-3',\n",
    "                  'poi_pc1', 'poi_pc2',\n",
    "                  'tj_lvl1', 'tj_lvl2', 'tj_lvl3',\n",
    "                  'ts_0', 'ts_1', 'ts_2', 'ts_3', 'ts_4', 'ts_5', 'ts_6', 'ts_7',\n",
    "                  'pollution', 'temperature',\n",
    "                  'wthr_0', 'wthr_1', 'wthr_2', 'wthr_3',\n",
    "                  'gap_t-1', 'gap_t-2', 'gap_t-3',\n",
    "                  'time_slot', 'week_day','district_id','poi_cluster'\n",
    "                 ]   \n",
    "\n",
    "gX_train = []\n",
    "gy_train = []\n",
    "gX_test  = []\n",
    "gy_test  = []\n",
    "\n",
    "# Use StandardScaler to achieve zero mean and unit variance\n",
    "# Generate two scalers: input and target\n",
    "input_scaler = StandardScaler().fit(pdata_set[gap_predictors])\n",
    "target_scaler = StandardScaler().fit(pdata_set['gap'])\n",
    "\n",
    "# Scale both training & test data\n",
    "gX_train  = input_scaler.transform(X_train[gap_predictors])\n",
    "gy_train  = target_scaler.transform(X_train['gap'])\n",
    "\n",
    "gX_test = input_scaler.transform(X_test[gap_predictors])\n",
    "gy_test = target_scaler.transform(X_test['gap'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENT FUNCTIONS FOR COMMON TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate Demand Supply Gap Metrics based on provided fit functions\n",
    "# Assumes expected values for gap is in namesake column\n",
    "def gap_estimate(**kwargs):\n",
    "    \"\"\"\n",
    "    Generate scores for gap.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gX_train  : array containing train features\n",
    "    gX_test   : array containing test features\n",
    "    g_fitfunc : function to use for predicting\n",
    "    \"\"\"\n",
    "    \n",
    "    gX_train     = kwargs[\"gX_train\"]\n",
    "    gX_test      = kwargs[\"gX_test\"]\n",
    "    g_fitfunc    = kwargs[\"g_fitfunc\"]\n",
    "    \n",
    "    print \"\\n\\nGAP FORECASTING\"\n",
    "    print     \"===============\"\n",
    "\n",
    "    # Generate predictions for train & test sets\n",
    "    gy_pred_train    = target_scaler.inverse_transform(g_fitfunc.predict(gX_train))\n",
    "    gy_pred_test     = target_scaler.inverse_transform(g_fitfunc.predict(gX_test))\n",
    "\n",
    "    # Extract expected train & test values\n",
    "    gy_train    = X_train['gap']\n",
    "    gy_test     = X_test['gap']\n",
    "\n",
    "    # Evaluate scores and print results\n",
    "    print_score(gy_train, gy_pred_train, gy_test, gy_pred_test) \n",
    "    return\n",
    "\n",
    "def print_score(y_train, y_pred_train, y_test, y_pred_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Present the MSE, R^2 and MAPE scores for train & test sets as a table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_train      : Array containing expected values for train set\n",
    "    y_pred_train : Array containing predicted values for train set\n",
    "    y_test       : Array containing expected values for test set\n",
    "    y_pred_test  : Array containing predicted values for test set\n",
    "    \"\"\"\n",
    "    \n",
    "    m2score_train    = metrics.mean_squared_error(y_train,    y_pred_train)\n",
    "    m2score_test     = metrics.mean_squared_error(y_test,     y_pred_test)\n",
    "\n",
    "\n",
    "    r2score_train    = metrics.r2_score(y_train,    y_pred_train)\n",
    "    r2score_test     = metrics.r2_score(y_test,     y_pred_test)\n",
    "\n",
    "    # Assumes data is for 144 time slots, 14 days (train), 7 days (test)\n",
    "    mpscore_train    = mape_score(y_train,    y_pred_train, ((144*14)-1))\n",
    "    mpscore_test     = mape_score(y_test,     y_pred_test, ((144*7)-1))\n",
    "\n",
    "\n",
    "    sets_list = [\"TRAIN\", \"TEST\"]\n",
    "\n",
    "    m2_scores = [m2score_train, m2score_test]\n",
    "    r2_scores = [r2score_train, r2score_test]\n",
    "    mp_scores = [mpscore_train, mpscore_test]\n",
    "\n",
    "\n",
    "    print '\\t\\tMEAN^2\\t\\tR2\\t\\tMAPE'\n",
    "\n",
    "    for s, m, r, mp in zip(sets_list, m2_scores, r2_scores, mp_scores):\n",
    "        print '{0:10}\\t{1:.3f}\\t\\t{2:.3f}\\t\\t{3:.3f}' .format(s, m, r, mp)\n",
    "\n",
    "\n",
    "def mape_score(exp, pred, q):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate the MAPE score value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    exp  : Array containing expected values\n",
    "    pred : Array containing predicted values\n",
    "    q    : Constant representing (number of days * number of time slots) - 1\n",
    "    \"\"\"\n",
    "    \n",
    "    mape = 0.0\n",
    "    n = 66.0\n",
    "    \n",
    "    for gap, gapX in zip(exp, pred):\n",
    "        if gap > 0:\n",
    "            mape += 1.0 * abs((gap-gapX)/gap)\n",
    "    return (mape/(n*q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a scorer function using the MAPE metric\n",
    "# Use the training data size (144 time slots * 14 days) for q value\n",
    "\n",
    "mape_scorer = make_scorer(mape_score, greater_is_better=False, q=((144*14)-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FORESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tMEAN^2\t\tR2\t\tMAPE\n",
      "TRAIN     \t126.596\t\t0.934\t\t0.329\n",
      "TEST      \t274.642\t\t0.881\t\t0.338\n"
     ]
    }
   ],
   "source": [
    "rf_predictors = [0,1,6,7,25,26,27,28]\n",
    "\n",
    "regressor = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=11, max_features=1.0, max_leaf_nodes=None, \n",
    "                                  min_samples_leaf=5, min_samples_split=15, min_weight_fraction_leaf=0.0, \n",
    "                                  n_estimators=300, n_jobs=-1, oob_score=False, random_state=0, verbose=0, \n",
    "                                  warm_start=False)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(gX_train[:, rf_predictors], gy_train)\n",
    "\n",
    "# Predict\n",
    "train_prediction = target_scaler.inverse_transform(regressor.predict(gX_train[:, rf_predictors]))\n",
    "test_prediction  = target_scaler.inverse_transform(regressor.predict(gX_test[:, rf_predictors]))\n",
    "\n",
    "# Score\n",
    "print_score(X_train['gap'], train_prediction, X_test['gap'], test_prediction) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT BOOSTED TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tMEAN^2\t\tR2\t\tMAPE\n",
      "TRAIN     \t40.489\t\t0.979\t\t0.334\n",
      "TEST      \t249.953\t\t0.892\t\t0.339\n"
     ]
    }
   ],
   "source": [
    "gbr_predictors = [0,1,2,8,9,10,25,26,27,28,29,30]\n",
    "\n",
    "regressor = GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.1, loss='ls',max_depth=5, max_features=None, \n",
    "                                      max_leaf_nodes=None, min_samples_leaf=3, min_samples_split=6, \n",
    "                                      min_weight_fraction_leaf=0.0, n_estimators=200, presort='auto', random_state=None, \n",
    "                                      subsample=1.0, verbose=0, warm_start=False)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(gX_train[:, gbr_predictors], gy_train)\n",
    "\n",
    "# Predict\n",
    "train_prediction = target_scaler.inverse_transform(regressor.predict(gX_train[:, gbr_predictors]))\n",
    "test_prediction  = target_scaler.inverse_transform(regressor.predict(gX_test[:, gbr_predictors]))\n",
    "\n",
    "# Score\n",
    "print_score(X_train['gap'], train_prediction, X_test['gap'], test_prediction) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
