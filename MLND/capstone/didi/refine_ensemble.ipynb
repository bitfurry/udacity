{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOUT THIS NOTEBOOK\n",
    "## Purpose\n",
    "This notebook uses the best machine learning models that were obtained after refinement and builds and ensemble model.    \n",
    "## Input\n",
    "'data_set.pickle' generated by 'data_processing.ipynb'.\n",
    "## Output\n",
    "Results of residual analysis.\n",
    "## Tasks Performed\n",
    "* Load library packages\n",
    "* Load pickle file\n",
    "* Split data into train & test sets\n",
    "    * Train: weeks 1 & 2, Test: week 3\n",
    "    * Perform feature scaling\n",
    "* Run the following algorithms:\n",
    "    * Random Forest Regressor\n",
    "    * Support Vector Machines\n",
    "    * Gradient Boosting Regressor\n",
    "    * Neural Networks\n",
    "* Average the results to generate an ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD LIBRARY PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in packages from os, numpy, pandas, matplotlib, seaborn, tensorflow, sklearn & six\n"
     ]
    }
   ],
   "source": [
    "# Import the required library packages\n",
    "import os\n",
    "import re\n",
    "import timeit\n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.learn as skflow\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Settings for matplotlib, Seaborn\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Set font sizes for matplots\n",
    "plt.rcParams.update({'font.size': 15, \n",
    "                     'legend.fontsize': 'medium', \n",
    "                     'axes.titlesize': 'medium', \n",
    "                     'axes.labelsize': 'medium'})\n",
    "\n",
    "print 'Read in packages from os, numpy, pandas, matplotlib, seaborn, tensorflow, sklearn & six'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD PICKLE FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ptrain_set (199584, 55)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'data_set.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    pdata_set = save['data_set']\n",
    "    del save\n",
    "    print 'Loaded ptrain_set', pdata_set.shape\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['district_id', 'num_day', 'time_slot', 'week_day', 'demand',\n",
       "       'demand_t-1', 'demand_t-2', 'demand_t-3', 'supply', 'supply_t-1',\n",
       "       'supply_t-2', 'supply_t-3', 'gap', 'weather', 'temperature',\n",
       "       'pollution', 'poi_pc1', 'poi_pc2', 'poi_pc3', 'poi_pc4',\n",
       "       'poi_cluster', 'tj_lvl1', 'tj_lvl2', 'tj_lvl3', 'tj_lvl4', 'dist_0',\n",
       "       'dist_1', 'dist_2', 'dist_3', 'dist_4', 'dist_5', 'dist_6',\n",
       "       'numday_0', 'numday_1', 'numday_2', 'numday_3', 'numday_4', 'ts_0',\n",
       "       'ts_1', 'ts_2', 'ts_3', 'ts_4', 'ts_5', 'ts_6', 'ts_7', 'weekday_0',\n",
       "       'weekday_1', 'weekday_2', 'poi_0', 'poi_1', 'poi_2', 'wthr_0',\n",
       "       'wthr_1', 'wthr_2', 'wthr_3'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdata_set.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD GAP FOR PREVIOUS TIME SLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new gap features for previous 3 time slots\n",
    "pdata_set['gap_t-1'] = (pdata_set['demand_t-1'] - pdata_set['supply_t-1'])\n",
    "pdata_set['gap_t-2'] = (pdata_set['demand_t-2'] - pdata_set['supply_t-2'])\n",
    "pdata_set['gap_t-3'] = (pdata_set['demand_t-3'] - pdata_set['supply_t-3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT DATA INTO TRAIN & TEST SETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use weeks 1 & 2 for training, week 3 for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train, X_test: (133056, 55) (66528, 55) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_days     = range(1,15)\n",
    "test_days      = range(15, 22)\n",
    "\n",
    "X_train     = pdata_set[(pdata_set['num_day'].isin(train_days))]\n",
    "X_test      = pdata_set[(pdata_set['num_day'].isin(test_days))]\n",
    "\n",
    "print \"Shape of X_train, X_test:\", X_train.shape, X_test.shape, \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate scaled features for train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "gap_predictors = ['demand_t-1', 'demand_t-2', 'demand_t-3',\n",
    "                  'supply_t-1', 'supply_t-2', 'supply_t-3',\n",
    "                  'poi_pc1', 'poi_pc2',\n",
    "                  'tj_lvl1', 'tj_lvl2', 'tj_lvl3',\n",
    "                  'ts_0', 'ts_1', 'ts_2', 'ts_3', 'ts_4', 'ts_5', 'ts_6', 'ts_7',\n",
    "                  'pollution', 'temperature',\n",
    "                  'wthr_0', 'wthr_1', 'wthr_2', 'wthr_3',\n",
    "                  'gap_t-1', 'gap_t-2', 'gap_t-3',\n",
    "                  'time_slot', 'week_day'\n",
    "                 ] \n",
    "gX_train = []\n",
    "gy_train = []\n",
    "gX_test  = []\n",
    "gy_test  = []\n",
    "\n",
    "# Use StandardScaler to achieve zero mean and unit variance\n",
    "# Generate two scalers: input and target\n",
    "input_scaler = StandardScaler().fit(pdata_set[gap_predictors])\n",
    "target_scaler = StandardScaler().fit(pdata_set['gap'])\n",
    "\n",
    "# Scale both training & test data\n",
    "gX_train  = input_scaler.transform(X_train[gap_predictors])\n",
    "gy_train  = target_scaler.transform(X_train['gap'])\n",
    "\n",
    "gX_test = input_scaler.transform(X_test[gap_predictors])\n",
    "gy_test = target_scaler.transform(X_test['gap'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define un-scaled features for train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gXu_train = []\n",
    "gyu_train = []\n",
    "gXu_test  = []\n",
    "gyu_test = []\n",
    "\n",
    "\n",
    "# Define training & test data frames\n",
    "gXu_train  = X_train[gap_predictors].as_matrix()\n",
    "gyu_train  = X_train['gap'].as_matrix()\n",
    "\n",
    "gXu_test = X_test[gap_predictors].as_matrix()\n",
    "gyu_test = X_test['gap'].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCORING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_score(y_train, y_pred_train, y_test, y_pred_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Present the MSE, R^2 and MAPE scores for train & test sets as a table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_train      : Array containing expected values for train set\n",
    "    y_pred_train : Array containing predicted values for train set\n",
    "    y_test       : Array containing expected values for test set\n",
    "    y_pred_test  : Array containing predicted values for test set\n",
    "    \"\"\"\n",
    "    \n",
    "    m2score_train    = metrics.mean_squared_error(y_train,    y_pred_train)\n",
    "    m2score_test     = metrics.mean_squared_error(y_test,     y_pred_test)\n",
    "\n",
    "\n",
    "    r2score_train    = metrics.r2_score(y_train,    y_pred_train)\n",
    "    r2score_test     = metrics.r2_score(y_test,     y_pred_test)\n",
    "\n",
    "    # Assumes data is for 144 time slots, 14 days (train), 7 days (test)\n",
    "    mpscore_train    = mape_score(y_train,    y_pred_train, ((144*14)-1))\n",
    "    mpscore_test     = mape_score(y_test,     y_pred_test, ((144*7)-1))\n",
    "\n",
    "\n",
    "    sets_list = [\"TRAIN\", \"TEST\"]\n",
    "\n",
    "    m2_scores = [m2score_train, m2score_test]\n",
    "    r2_scores = [r2score_train, r2score_test]\n",
    "    mp_scores = [mpscore_train, mpscore_test]\n",
    "\n",
    "\n",
    "    print '\\t\\tMEAN^2\\t\\tR2\\t\\tMAPE'\n",
    "\n",
    "    for s, m, r, mp in zip(sets_list, m2_scores, r2_scores, mp_scores):\n",
    "        print '{0:10}\\t{1:.3f}\\t\\t{2:.3f}\\t\\t{3:.3f}' .format(s, m, r, mp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FORESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_predictors = [0,3,1,5,2,8,4,19,25,26,27,28,29]\n",
    "\n",
    "regressor = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=12, max_features=1.0, max_leaf_nodes=None, \n",
    "                                  min_samples_leaf=5, min_samples_split=15, min_weight_fraction_leaf=0.0, n_estimators=300, \n",
    "                                  n_jobs=1, oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(gX_train[:, rf_predictors], gy_train)\n",
    "\n",
    "# Predict\n",
    "rf_trainpred = target_scaler.inverse_transform(regressor.predict(gX_train[:, rf_predictors]))\n",
    "rf_testpred  = target_scaler.inverse_transform(regressor.predict(gX_test[:, rf_predictors]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPORT VECTOR MACHINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_predictors = [0,3,1,4,25,26,28,29]\n",
    "\n",
    "regressor = SVR(kernel='linear', C=1.0, epsilon=0.1, cache_size=10000)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(gX_train[:, svm_predictors], gy_train)\n",
    "\n",
    "# Predict\n",
    "svm_trainpred = target_scaler.inverse_transform(regressor.predict(gX_train[:, svm_predictors]))\n",
    "svm_testpred  = target_scaler.inverse_transform(regressor.predict(gX_test[:, svm_predictors]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT BOOSTED TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbr_predictors = [0,3,2,1,20,9,5,19,25,26,27,28,29] \n",
    "\n",
    "regressor = GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.15, loss='ls',max_depth=5, max_features=None, \n",
    "                                max_leaf_nodes=None, min_samples_leaf=3, min_samples_split=8, min_weight_fraction_leaf=0.0, \n",
    "                                n_estimators=100, presort='auto', random_state=None, subsample=0.8, verbose=0,\n",
    "                                warm_start=False)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(gXu_train[:, gbr_predictors], gyu_train)\n",
    "\n",
    "# Predict\n",
    "gbr_trainpred = regressor.predict(gXu_train[:, gbr_predictors])\n",
    "gbr_testpred  = regressor.predict(gXu_test[:, gbr_predictors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn_predictors = [0,3,1,5,2,8,4,19,25,26,27,28,29]\n",
    "\n",
    "# Set up optimizer, regressor\n",
    "learning_rate=0.01 \n",
    "hidden_units=[11] \n",
    "dropout=0.3 \n",
    "steps=50000 \n",
    "batch_size=3000\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "regressor = skflow.DNNRegressor(hidden_units=hidden_units, optimizer=optimizer, dropout=float(dropout))\n",
    "\n",
    "# Fit\n",
    "regressor.fit(gX_train[:, nn_predictors], gy_train, steps=steps, batch_size=batch_size)\n",
    "\n",
    "# Predict\n",
    "nn_trainpred = target_scaler.inverse_transform(regressor.predict(gX_train[:, nn_predictors]))\n",
    "nn_testpred  = target_scaler.inverse_transform(regressor.predict(gX_test[:, nn_predictors]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_prediction = (rf_trainpred + svm_trainpred + gbr_trainpred + nn_trainpred) / 4.0\n",
    "test_prediction  = (rf_testpred + svm_testpred + gbr_testpred + nn_testpred) / 4.0\n",
    "print_score(gy_train, train_prediction, gy_test, test_prediction) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
