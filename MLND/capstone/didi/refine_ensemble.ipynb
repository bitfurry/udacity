{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOUT THIS NOTEBOOK\n",
    "## Purpose\n",
    "This notebook uses the best machine learning models that were obtained after refinement and builds and ensemble model.    \n",
    "## Input\n",
    "'data_set.pickle' generated by 'data_processing.ipynb'.\n",
    "## Output\n",
    "Results of residual analysis.\n",
    "## Tasks Performed\n",
    "* Load library packages\n",
    "* Load pickle file\n",
    "* Split data into train & test sets\n",
    "    * Train: weeks 1 & 2, Test: week 3\n",
    "    * Perform feature scaling\n",
    "* Run the following algorithms:\n",
    "    * Random Forest Regressor\n",
    "    * Support Vector Machines\n",
    "    * Gradient Boosting Regressor\n",
    "    * Neural Networks\n",
    "* Average the results to generate an ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD LIBRARY PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in packages from os, numpy, pandas, matplotlib, seaborn, tensorflow, sklearn & six\n"
     ]
    }
   ],
   "source": [
    "# Import the required library packages\n",
    "import os\n",
    "import re\n",
    "import timeit\n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.learn as skflow\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Settings for matplotlib, Seaborn\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Set font sizes for matplots\n",
    "plt.rcParams.update({'font.size': 15, \n",
    "                     'legend.fontsize': 'medium', \n",
    "                     'axes.titlesize': 'medium', \n",
    "                     'axes.labelsize': 'medium'})\n",
    "\n",
    "print 'Read in packages from os, numpy, pandas, matplotlib, seaborn, tensorflow, sklearn & six'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD PICKLE FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ptrain_set (199584, 55)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'data_set.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    pdata_set = save['data_set']\n",
    "    del save\n",
    "    print 'Loaded ptrain_set', pdata_set.shape\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['district_id', 'num_day', 'time_slot', 'week_day', 'demand',\n",
       "       'demand_t-1', 'demand_t-2', 'demand_t-3', 'supply', 'supply_t-1',\n",
       "       'supply_t-2', 'supply_t-3', 'gap', 'weather', 'temperature',\n",
       "       'pollution', 'poi_pc1', 'poi_pc2', 'poi_pc3', 'poi_pc4',\n",
       "       'poi_cluster', 'tj_lvl1', 'tj_lvl2', 'tj_lvl3', 'tj_lvl4', 'dist_0',\n",
       "       'dist_1', 'dist_2', 'dist_3', 'dist_4', 'dist_5', 'dist_6',\n",
       "       'numday_0', 'numday_1', 'numday_2', 'numday_3', 'numday_4', 'ts_0',\n",
       "       'ts_1', 'ts_2', 'ts_3', 'ts_4', 'ts_5', 'ts_6', 'ts_7', 'weekday_0',\n",
       "       'weekday_1', 'weekday_2', 'poi_0', 'poi_1', 'poi_2', 'wthr_0',\n",
       "       'wthr_1', 'wthr_2', 'wthr_3'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdata_set.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD GAP FOR PREVIOUS TIME SLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new gap features for previous 3 time slots\n",
    "pdata_set['gap_t-1'] = (pdata_set['demand_t-1'] - pdata_set['supply_t-1'])\n",
    "pdata_set['gap_t-2'] = (pdata_set['demand_t-2'] - pdata_set['supply_t-2'])\n",
    "pdata_set['gap_t-3'] = (pdata_set['demand_t-3'] - pdata_set['supply_t-3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT DATA INTO TRAIN & TEST SETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use weeks 1 & 2 for training, week 3 for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train, X_test: (133056, 58) (66528, 58) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_days     = range(1,15)\n",
    "test_days      = range(15, 22)\n",
    "\n",
    "X_train     = pdata_set[(pdata_set['num_day'].isin(train_days))]\n",
    "X_test      = pdata_set[(pdata_set['num_day'].isin(test_days))]\n",
    "\n",
    "print \"Shape of X_train, X_test:\", X_train.shape, X_test.shape, \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate scaled features for train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "gap_predictors = ['demand_t-1', 'demand_t-2', 'demand_t-3',\n",
    "                  'supply_t-1', 'supply_t-2', 'supply_t-3',\n",
    "                  'poi_pc1', 'poi_pc2',\n",
    "                  'tj_lvl1', 'tj_lvl2', 'tj_lvl3',\n",
    "                  'ts_0', 'ts_1', 'ts_2', 'ts_3', 'ts_4', 'ts_5', 'ts_6', 'ts_7',\n",
    "                  'pollution', 'temperature',\n",
    "                  'wthr_0', 'wthr_1', 'wthr_2', 'wthr_3',\n",
    "                  'gap_t-1', 'gap_t-2', 'gap_t-3',\n",
    "                  'time_slot', 'week_day'\n",
    "                 ] \n",
    "gX_train = []\n",
    "gy_train = []\n",
    "gX_test  = []\n",
    "gy_test  = []\n",
    "\n",
    "# Use StandardScaler to achieve zero mean and unit variance\n",
    "# Generate two scalers: input and target\n",
    "input_scaler = StandardScaler().fit(pdata_set[gap_predictors])\n",
    "target_scaler = StandardScaler().fit(pdata_set['gap'])\n",
    "\n",
    "# Scale both training & test data\n",
    "gX_train  = input_scaler.transform(X_train[gap_predictors])\n",
    "gy_train  = target_scaler.transform(X_train['gap'])\n",
    "\n",
    "gX_test = input_scaler.transform(X_test[gap_predictors])\n",
    "gy_test = target_scaler.transform(X_test['gap'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCORING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_score(y_train, y_pred_train, y_test, y_pred_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Present the MSE, R^2 and MAPE scores for train & test sets as a table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_train      : Array containing expected values for train set\n",
    "    y_pred_train : Array containing predicted values for train set\n",
    "    y_test       : Array containing expected values for test set\n",
    "    y_pred_test  : Array containing predicted values for test set\n",
    "    \"\"\"\n",
    "    \n",
    "    m2score_train    = metrics.mean_squared_error(y_train,    y_pred_train)\n",
    "    m2score_test     = metrics.mean_squared_error(y_test,     y_pred_test)\n",
    "\n",
    "\n",
    "    r2score_train    = metrics.r2_score(y_train,    y_pred_train)\n",
    "    r2score_test     = metrics.r2_score(y_test,     y_pred_test)\n",
    "\n",
    "    # Assumes data is for 144 time slots, 14 days (train), 7 days (test)\n",
    "    mpscore_train    = mape_score(y_train,    y_pred_train, ((144*14)-1))\n",
    "    mpscore_test     = mape_score(y_test,     y_pred_test, ((144*7)-1))\n",
    "\n",
    "\n",
    "    sets_list = [\"TRAIN\", \"TEST\"]\n",
    "\n",
    "    m2_scores = [m2score_train, m2score_test]\n",
    "    r2_scores = [r2score_train, r2score_test]\n",
    "    mp_scores = [mpscore_train, mpscore_test]\n",
    "\n",
    "\n",
    "    print '\\t\\tMEAN^2\\t\\tR2\\t\\tMAPE'\n",
    "\n",
    "    for s, m, r, mp in zip(sets_list, m2_scores, r2_scores, mp_scores):\n",
    "        print '{0:10}\\t{1:.3f}\\t\\t{2:.3f}\\t\\t{3:.3f}' .format(s, m, r, mp)\n",
    "        \n",
    "def mape_score(exp, pred, q):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate the MAPE score value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    exp  : Array containing expected values\n",
    "    pred : Array containing predicted values\n",
    "    q    : Constant representing (number of days * number of time slots) - 1\n",
    "    \"\"\"\n",
    "    \n",
    "    mape = 0.0\n",
    "    n = 66.0\n",
    "    \n",
    "    for gap, gapX in zip(exp, pred):\n",
    "        if gap > 0:\n",
    "            mape += 1.0 * abs((gap-gapX)/gap)\n",
    "    return (mape/(n*q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FORESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tMEAN^2\t\tR2\t\tMAPE\n",
      "TRAIN     \t119.058\t\t0.938\t\t0.323\n",
      "TEST      \t281.312\t\t0.878\t\t0.340\n"
     ]
    }
   ],
   "source": [
    "rf_predictors = [0,3,1,5,2,8,4,19,25,26,27,28,29]\n",
    "\n",
    "regressor = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=12, max_features=1.0, max_leaf_nodes=None, \n",
    "                                  min_samples_leaf=5, min_samples_split=15, min_weight_fraction_leaf=0.0, n_estimators=300, \n",
    "                                  n_jobs=-1, oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(gX_train[:, rf_predictors], gy_train)\n",
    "\n",
    "# Predict\n",
    "rf_trainpred = target_scaler.inverse_transform(regressor.predict(gX_train[:, rf_predictors]))\n",
    "rf_testpred  = target_scaler.inverse_transform(regressor.predict(gX_test[:, rf_predictors]))\n",
    "\n",
    "# Score\n",
    "print_score(X_train['gap'], rf_trainpred, X_test['gap'], rf_testpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-7275524a07f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrf_predictors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgy_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    288\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 290\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    808\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 810\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    811\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    725\u001b[0m                 \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m                 \u001b[1;31m# Stop dispatching any new job in the async callback thread\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s.wait(): got it\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rf_predictors = [0,1,3,4,6,7,8,9,10,25,26,28,29]\n",
    "\n",
    "regressor = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=11, max_features=0.8, max_leaf_nodes=None, \n",
    "                                  min_samples_leaf=5, min_samples_split=15, min_weight_fraction_leaf=0.0, \n",
    "                                  n_estimators=20000, n_jobs=-1, oob_score=False, random_state=0, verbose=0, \n",
    "                                  warm_start=False)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(gX_train[:, rf_predictors], gy_train)\n",
    "\n",
    "# Predict\n",
    "rf2_trainpred = target_scaler.inverse_transform(regressor.predict(gX_train[:, rf_predictors]))\n",
    "rf2_testpred  = target_scaler.inverse_transform(regressor.predict(gX_test[:, rf_predictors]))\n",
    "\n",
    "# Score\n",
    "print_score(X_train['gap'], rf2_trainpred, X_test['gap'], rf2_testpred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPORT VECTOR MACHINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_predictors = [0,3,1,4,25,26,28,29]\n",
    "\n",
    "regressor = SVR(kernel='linear', C=1.0, epsilon=0.1, cache_size=10000)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(gX_train[:, svm_predictors], gy_train)\n",
    "\n",
    "# Predict\n",
    "svm_trainpred = target_scaler.inverse_transform(regressor.predict(gX_train[:, svm_predictors]))\n",
    "svm_testpred  = target_scaler.inverse_transform(regressor.predict(gX_test[:, svm_predictors]))\n",
    "\n",
    "# Score\n",
    "print_score(X_train, svm_trainpred, gyu_test, svm_testpred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT BOOSTED TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbr_predictors = [0,3,2,1,20,9,5,19,25,26,27,28,29] \n",
    "\n",
    "regressor = GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.15, loss='ls',max_depth=5, max_features=None, \n",
    "                                max_leaf_nodes=None, min_samples_leaf=3, min_samples_split=8, min_weight_fraction_leaf=0.0, \n",
    "                                n_estimators=100, presort='auto', random_state=None, subsample=0.8, verbose=0,\n",
    "                                warm_start=False)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(gXu_train[:, gbr_predictors], gyu_train)\n",
    "\n",
    "# Predict\n",
    "gbr_trainpred = regressor.predict(gXu_train[:, gbr_predictors])\n",
    "gbr_testpred  = regressor.predict(gXu_test[:, gbr_predictors])\n",
    "\n",
    "# Score\n",
    "print_score(gyu_train, gbr_trainpred, gyu_test, gbr_testpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbr_predictors = [0,3,2,1,20,9,5,19,25,26,27,28,29] \n",
    "\n",
    "regressor = GradientBoostingRegressor(alpha=0.9, init=None, learning_rate=0.15, loss='ls',max_depth=5, max_features=None, \n",
    "                                max_leaf_nodes=None, min_samples_leaf=3, min_samples_split=8, min_weight_fraction_leaf=0.0, \n",
    "                                n_estimators=100, presort='auto', random_state=None, subsample=0.8, verbose=0,\n",
    "                                warm_start=False)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(gXu_train[:, gbr_predictors], gyu_train)\n",
    "\n",
    "# Predict\n",
    "gbr2_trainpred = regressor.predict(gXu_train[:, gbr_predictors])\n",
    "gbr2_testpred  = regressor.predict(gXu_test[:, gbr_predictors])\n",
    "\n",
    "# Score\n",
    "print_score(gyu_train, gbr2_trainpred, gyu_test, gbr2_testpred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tMEAN^2\t\tR2\t\tMAPE\n",
      "TRAIN     \t202.553\t\t0.894\t\t0.393\n",
      "TEST      \t334.087\t\t0.856\t\t0.379\n"
     ]
    }
   ],
   "source": [
    "nn_predictors = [0,3,1,5,2,8,4,19,25,26,27,28,29]\n",
    "\n",
    "# Set up optimizer, regressor\n",
    "learning_rate=0.01 \n",
    "hidden_units=[11] \n",
    "dropout=0.3 \n",
    "steps=50000 \n",
    "batch_size=3000\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "regressor = skflow.DNNRegressor(hidden_units=hidden_units, optimizer=optimizer, dropout=float(dropout))\n",
    "\n",
    "# Fit\n",
    "regressor.fit(gX_train[:, nn_predictors], gy_train, steps=steps, batch_size=batch_size)\n",
    "\n",
    "# Predict\n",
    "nn_trainpred = target_scaler.inverse_transform(regressor.predict(gX_train[:, nn_predictors]))\n",
    "nn_testpred  = target_scaler.inverse_transform(regressor.predict(gX_test[:, nn_predictors]))\n",
    "\n",
    "# Score\n",
    "print_score(gyu_train, nn_trainpred, gyu_test, nn_testpred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tMEAN^2\t\tR2\t\tMAPE\n",
      "TRAIN     \t116.682\t\t0.939\t\t0.344\n",
      "TEST      \t270.816\t\t0.883\t\t0.347\n"
     ]
    }
   ],
   "source": [
    "train_prediction = (rf_trainpred + svm_trainpred + gbr_trainpred + nn_trainpred) / 4.0\n",
    "test_prediction  = (rf_testpred + svm_testpred + gbr_testpred + nn_testpred) / 4.0\n",
    "print_score(gyu_train, train_prediction, gyu_test, test_prediction) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
