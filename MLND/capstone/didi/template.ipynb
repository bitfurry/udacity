{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in packages from numpy, pandas, sklearn, seaborn & matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "print \"Read in packages from numpy, pandas, sklearn, seaborn & matplotlib\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD TRAINING, TEST & FINAL TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load training data\n",
    "dates_ = range(1, 22)\n",
    "dates = [\"{:02d}\".format(item) for item in dates_] \n",
    "\n",
    "# Read in order data\n",
    "order_train = pd.concat( pd.read_table('../../../../../../data/season_2/training_data/order_data/order_data_2016-01-%s' %i, header=None, names = ['order_id', 'driver_id', 'passenger_id', 'start_district_hash', 'dest_district_hash', 'Price', 'Time' ])\n",
    "                      for i in dates)\n",
    "\n",
    "# Convert to datetime\n",
    "order_train['Time'] = pd.to_datetime(order_train['Time'])  \n",
    "\n",
    "# Read in poi data\n",
    "poi_train = pd.read_table('../../../../../../data/season_2/training_data/poi_data/poi_data', sep=' ', header=None, names = ['district_hash'])\n",
    "poi_train = pd.DataFrame(poi_train.district_hash.str.split('\\W+',1).tolist(), columns = ['district_hash','poi_class'])\n",
    "\n",
    "# Read in traffic data\n",
    "traffic_train = pd.concat( pd.read_table('../../../../../../data/season_2/training_data/traffic_data/traffic_data_2016-01-%s' %i, sep=' ', header=None, names = ['district_hash_orig', 'time'])\n",
    "                        for i in dates)\n",
    "\n",
    "split1 = pd.DataFrame(traffic_train.district_hash_orig.str.split('[\\W+]',1).tolist())\n",
    "traffic_train['district_hash'] = split1[0]\n",
    "traffic_train['tj'] = split1[1]\n",
    "\n",
    "split2 = pd.DataFrame(traffic_train.tj.str.split('(\\d{4}\\-\\d{2}\\-\\d{2}$)',1).tolist())\n",
    "traffic_train['tj_level'] = split2[0]\n",
    "traffic_train['date'] = split2[1]\n",
    "\n",
    "traffic_train['Time'] = traffic_train.apply(lambda r: str(r.date) + ' '+ str(r.time), axis=1)\n",
    "traffic_train.drop(['district_hash_orig', 'time', 'tj', 'date'], axis=1, inplace=True)\n",
    "\n",
    "# Convert to datetime\n",
    "traffic_train['Time'] = pd.to_datetime(traffic_train['Time'])\n",
    "\n",
    "# Read in weather data                                                                                                                                                            \n",
    "weather_train = pd.concat( pd.read_table('../../../../../../data/season_2/training_data/weather_data/weather_data_2016-01-%s' %i, header=None, names = ['Time', 'Weather', 'temperature', 'PM2.5' ])\n",
    "                        for i in dates)                      \n",
    "\n",
    "# Convert to datetime\n",
    "weather_train['Time'] = pd.to_datetime(weather_train['Time']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "#dates = ('22_test', '24_test', '26_test', '28_test', '30_test')\n",
    "dates = ('23_test', '25_test', '27_test', '29_test', '31_test')\n",
    "\n",
    "# Load cluster map - generate dictionary to map district_hash to district_id\n",
    "district = pd.read_table('../../../../../../data/season_2/test_set_2/cluster_map/cluster_map', sep=' ', header=None, names=['district_hash'])\n",
    "district = pd.DataFrame(district.district_hash.str.split('\\W+',1).tolist(), columns = ['district_hash', 'district_id'])\n",
    "district_dict = district.set_index('district_hash')['district_id'].to_dict()\n",
    "\n",
    "# Read in order data\n",
    "order_test = pd.concat( pd.read_table('../../../../../../data/season_2/test_set_2/order_data/order_data_2016-01-%s' %i, header=None,\n",
    "                                      names = ['order_id', 'driver_id', 'passenger_id', 'start_district_hash', 'dest_district_hash', 'Price', 'Time' ])\n",
    "                      for i in dates)\n",
    "\n",
    "# Convert to datetime\n",
    "order_test['Time'] = pd.to_datetime(order_test['Time'])   \n",
    "\n",
    "# Read in poi data\n",
    "poi_test = pd.read_table('../../../../../../data/season_2/test_set_2/poi_data/poi_data', sep=' ', header=None, names=['district_hash'])\n",
    "poi_test = pd.DataFrame(poi_test.district_hash.str.split('\\W+',1).tolist(), columns = ['district_hash','poi_class'])\n",
    "\n",
    "# Read in traffic data\n",
    "traffic_test = pd.concat( pd.read_table('../../../../../../data/season_2/test_set_2/traffic_data/traffic_data_2016-01-%s' %i, sep=' ',\n",
    "                                        header=None, names = ['district_hash_orig', 'time'])\n",
    "                        for i in dates)\n",
    "\n",
    "split1 = pd.DataFrame(traffic_test.district_hash_orig.str.split('[\\W+]',1).tolist())\n",
    "traffic_test['district_hash'] = split1[0]\n",
    "traffic_test['tj'] = split1[1]\n",
    "\n",
    "split2 = pd.DataFrame(traffic_test.tj.str.split('(\\d{4}\\-\\d{2}\\-\\d{2}$)',1).tolist())\n",
    "traffic_test['tj_level'] = split2[0]\n",
    "traffic_test['date'] = split2[1]\n",
    "\n",
    "traffic_test['Time'] = traffic_test.apply(lambda r: str(r.date) + ' '+ str(r.time), axis=1)\n",
    "traffic_test.drop(['district_hash_orig', 'time', 'tj', 'date'], axis=1, inplace=True)\n",
    "\n",
    "# Convert to datetime\n",
    "traffic_test['Time'] = pd.to_datetime(traffic_test['Time'])\n",
    "\n",
    "# Read in weather data    \n",
    "weather_test = pd.concat( pd.read_table('../../../../../../data/season_2/test_set_2/weather_data/weather_data_2016-01-%s' %i, \n",
    "                                        header=None, names = ['Time', 'Weather', 'temperature', 'PM2.5' ])\n",
    "                        for i in dates) \n",
    "\n",
    "# Convert to datetime\n",
    "weather_test['Time'] = pd.to_datetime(weather_test['Time'])                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in final_test df, scale features, generate predictions & write result file\n",
    "# Load readme - generate final_test df\n",
    "final_test = pd.read_table('../../../../../../data/season_2/test_set_2/read_me_2.txt', sep=' ', \n",
    "                           header=None, names=['time_stamp'])\n",
    "\n",
    "final_test = pd.DataFrame(final_test.time_stamp.str.split('-').tolist(), \n",
    "                          columns = ['year', 'month', 'day', 'time_slot'])\n",
    "\n",
    "# drop first row with Chinese characters\n",
    "final_test = final_test.ix[1:]\n",
    "\n",
    "day_test = final_test['day']\n",
    "time_test = final_test['time_slot']\n",
    "\n",
    "\n",
    "# Create train_set\n",
    "final_test_set = pd.DataFrame()\n",
    "\n",
    "for district in range(1,67):\n",
    "    for day, time in zip(day_test, time_test):\n",
    "        data = {'district_id': [district], \n",
    "                'num_day': [day], \n",
    "                'time_slot': [time],\n",
    "                'week_day': [(int(day)+3)%7]\n",
    "                }\n",
    "        df = pd.DataFrame(data, columns=['district_id', 'num_day', 'time_slot', 'week_day'])\n",
    "        final_test_set = final_test_set.append(df)\n",
    "\n",
    "# Feature scaling\n",
    "#VRfinal_test_transformed  = scaler.transform(test_set[predictors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2838, 4)\n"
     ]
    }
   ],
   "source": [
    "print final_test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######### FUNCTION DEFINITIONS ####################################################\n",
    "# Map district_hash to district_id\n",
    "def dist_hash2id(data_frame, hash_col, district_dict):\n",
    "    idvalue_array = []\n",
    "    \n",
    "    for hash_value in data_frame[hash_col]:\n",
    "        idvalue_array.append(district_dict[hash_value])\n",
    "    \n",
    "    return idvalue_array\n",
    "\n",
    "# Map time_stamp to time_slot\n",
    "def time_stamp2slot(arr_hour, arr_minute, arr_second):\n",
    "    time_slot = []\n",
    "    \n",
    "    for hour, minu, sec in zip(arr_hour, arr_minute, arr_second):\n",
    "        time_slot.append(int((6*hour + 0.1*minu + sec/600) + 1))\n",
    "    \n",
    "    return time_slot\n",
    "\n",
    "# Combine num_day and time_slot to create time_axis\n",
    "def time_axis(num_day, time_slot):\n",
    "    return (((num_day - 1)*144) + time_slot)\n",
    "\n",
    "\n",
    "# Calculate demand, supply and gap per time axis slot\n",
    "# Demand = Number of orders received (ride requests)\n",
    "# Supply = Number of orders answered\n",
    "# Gap = Number of orders missed (NA)\n",
    "def demand_supply(order_frame):\n",
    "    \n",
    "    demand = {}\n",
    "    supply = {}\n",
    "    gap = {}\n",
    "    \n",
    "    # Initialize demand & supply to zero\n",
    "    for district in range(1,67):\n",
    "        for time in range(1,31*144):\n",
    "            demand[(district, time)] = 0;\n",
    "            supply[(district, time)] = 0;\n",
    "\n",
    "    # Iterate over all orders to accumulate demand, supply count for each time slot\n",
    "    for district_id, num_day, time_slot, answered in zip(order_frame['district_id'], order_frame['num_day'], \n",
    "                                                         order_frame['time_slot'], order_frame['driver_id']):\n",
    "        demand[(int(district_id),time_axis(num_day, time_slot))] += 1;\n",
    "        supply[(int(district_id),time_axis(num_day, time_slot))] += pd.notnull(answered);\n",
    "        \n",
    "    # Calculate the demand-supply gap\n",
    "    for district in range(1,67):\n",
    "        for time in range(1,31*144):\n",
    "            gap[(district, time)] = demand[(district, time)] - supply[(district, time)];\n",
    "    \n",
    "    return (demand, supply, gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################        TRAINING DATA       #############################\n",
    "# Update ORDER\n",
    "# IGNORING: order_id, driver_id, passenger_id\n",
    "\n",
    "# Create following features - time_slot (1...144), week_day (1...7), num_day(1...31) \n",
    "order_train.set_index('Time', inplace=True)\n",
    "\n",
    "order_train['num_day'] = order_train.index.day\n",
    "order_train['time_slot'] = time_stamp2slot(order_train.index.hour, order_train.index.minute, order_train.index.second)\n",
    "\n",
    "order_train.reset_index(inplace='True')\n",
    "\n",
    "order_train['week_day'] = order_train['Time'].dt.dayofweek\n",
    "\n",
    "# Map start_district_hash (1...66)\n",
    "order_train['district_id'] = dist_hash2id(order_train, 'start_district_hash', district_dict)\n",
    "\n",
    "# Calculate demand, supply & gap for all district_id & time_slot\n",
    "d_train = {}\n",
    "s_train = {}\n",
    "g_train = {}\n",
    "d_train, s_train, g_train = demand_supply(order_train)\n",
    "\n",
    "# Map end_district_hash (1...381)\n",
    "\n",
    "###################################################################################\n",
    "# Update POI\n",
    "\n",
    "# Map district_hash (1...66) to district_id\n",
    "poi_train['district_id'] = dist_hash2id(poi_train, 'district_hash', district_dict)\n",
    "\n",
    "# Create sub features for poi_class\n",
    "\n",
    "###################################################################################\n",
    "# Update TRAFFIC\n",
    "\n",
    "# Create following features - time_slot (1...144), week_day (1...7), num_day(1...31) \n",
    "traffic_train.set_index('Time', inplace=True)\n",
    "\n",
    "traffic_train['num_day'] = traffic_train.index.day\n",
    "traffic_train['time_slot'] = time_stamp2slot(traffic_train.index.hour, traffic_train.index.minute, traffic_train.index.second)\n",
    "\n",
    "traffic_train.reset_index(inplace='True')\n",
    "\n",
    "traffic_train['week_day'] = traffic_train['Time'].dt.dayofweek\n",
    "\n",
    "# Map district_hash (1...66) to district_id\n",
    "traffic_train['district_id'] = dist_hash2id(traffic_train, 'district_hash', district_dict)\n",
    "\n",
    "# Create sub features for tj_level\n",
    "\n",
    "###################################################################################\n",
    "# Update WEATHER\n",
    "\n",
    "# Create following features - time_slot (1...144), week_day (1...7), num_day(1...31) \n",
    "weather_train.set_index('Time', inplace=True)\n",
    "\n",
    "weather_train['num_day'] = weather_train.index.day\n",
    "weather_train['time_slot'] = time_stamp2slot(weather_train.index.hour, weather_train.index.minute, weather_train.index.second)\n",
    "\n",
    "weather_train.reset_index(inplace='True')\n",
    "\n",
    "weather_train['week_day'] = weather_train['Time'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#######################        TEST DATA        ##############################\n",
    "# Update ORDER\n",
    "# IGNORING: order_id, driver_id, passenger_id\n",
    "\n",
    "# Create following features - time_slot (1...144), week_day (1...7), num_day(1...31) \n",
    "\n",
    "order_test.set_index('Time', inplace=True)\n",
    "\n",
    "order_test['num_day'] = order_test.index.day\n",
    "order_test['time_slot'] = time_stamp2slot(order_test.index.hour, order_test.index.minute, order_test.index.second)\n",
    "\n",
    "order_test.reset_index(inplace='True')\n",
    "\n",
    "order_test['week_day'] = order_test['Time'].dt.dayofweek\n",
    "\n",
    "# Map start_district_hash (1...66)\n",
    "order_test['district_id'] = dist_hash2id(order_test, 'start_district_hash', district_dict)\n",
    "\n",
    "# Calculate demand, supply & gap for all district_id & time_slot\n",
    "d_test = {}\n",
    "s_test = {}\n",
    "g_test = {}\n",
    "d_test, s_test, g_test = demand_supply(order_test)\n",
    "\n",
    "# Map end_district_hash (1...381)\n",
    "\n",
    "###################################################################################\n",
    "# Update POI\n",
    "\n",
    "# Map district_hash (1...66) to district_id\n",
    "poi_test['district_id'] = dist_hash2id(poi_test, 'district_hash', district_dict)\n",
    "\n",
    "# Create sub features for poi_class\n",
    "\n",
    "###################################################################################\n",
    "# Update TRAFFIC\n",
    "\n",
    "# Create following features - time_slot (1...144), week_day (1...7), num_day(1...31) \n",
    "traffic_test.set_index('Time', inplace=True)\n",
    "\n",
    "traffic_test['num_day'] = traffic_test.index.day\n",
    "traffic_test['time_slot'] = time_stamp2slot(traffic_test.index.hour, traffic_test.index.minute, traffic_test.index.second)\n",
    "\n",
    "traffic_test.reset_index(inplace='True')\n",
    "\n",
    "traffic_test['week_day'] = traffic_test['Time'].dt.dayofweek\n",
    "\n",
    "# Map district_hash (1...66) to district_id\n",
    "traffic_test['district_id'] = dist_hash2id(traffic_test, 'district_hash', district_dict)\n",
    "\n",
    "# Create sub features for tj_level\n",
    "\n",
    "###################################################################################\n",
    "# Update WEATHER\n",
    "\n",
    "# Create following features - time_slot (1...144), week_day (1...7), num_day(1...31) \n",
    "weather_test.set_index('Time', inplace=True)\n",
    "\n",
    "weather_test['num_day'] = weather_test.index.day\n",
    "weather_test['time_slot'] = time_stamp2slot(weather_test.index.hour, weather_test.index.minute, weather_test.index.second)\n",
    "\n",
    "weather_test.reset_index(inplace='True')\n",
    "\n",
    "weather_test['week_day'] = weather_test['Time'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATE TRAIN SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create train_set\n",
    "train_set = pd.DataFrame()\n",
    "\n",
    "for district in range(1,67):\n",
    "    # Skip Jan 1 (Holiday)\n",
    "    for day in range(2,22):\n",
    "        for time in range(1,145):\n",
    "            data = {'district_id': [district], 'num_day': [day], 'time_slot': [time],\n",
    "                    'week_day': [(day+3)%7],\n",
    "                    'demand': d_train[(district,time_axis(day,time))],\n",
    "                    'supply': d_train[(district,time_axis(day,time))],\n",
    "                    'gap': g_train[(district,time_axis(day,time))]\n",
    "                   }\n",
    "            df = pd.DataFrame(data, columns=['district_id', 'num_day', 'time_slot', 'week_day', 'demand',\n",
    "                                             'supply', 'gap'])\n",
    "            train_set = train_set.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set: (190080, 7) \n",
      "\n",
      "\n",
      "Column Headers: ['district_id', 'num_day', 'time_slot', 'week_day', 'demand', 'supply', 'gap'] \n",
      "\n",
      "\n",
      "district_id    int64\n",
      "num_day        int64\n",
      "time_slot      int64\n",
      "week_day       int64\n",
      "demand         int64\n",
      "supply         int64\n",
      "gap            int64\n",
      "dtype: object \n",
      "\n",
      "\n",
      "   district_id  num_day  time_slot  week_day  demand  supply  gap\n",
      "0            1        2          1         5      88      88    3\n",
      "0            1        2          2         5      97      97    7\n",
      "0            1        2          3         5      75      75    5\n",
      "0            1        2          4         5      60      60    2\n",
      "0            1        2          5         5      72      72    7 \n",
      "\n",
      "\n",
      "         district_id        num_day      time_slot       week_day  \\\n",
      "count  190080.000000  190080.000000  190080.000000  190080.000000   \n",
      "mean       33.500000      11.500000      72.500000       2.950000   \n",
      "std        19.050422       5.766296      41.568326       2.036547   \n",
      "min         1.000000       2.000000       1.000000       0.000000   \n",
      "25%        17.000000       6.750000      36.750000       1.000000   \n",
      "50%        33.500000      11.500000      72.500000       3.000000   \n",
      "75%        50.000000      16.250000     108.250000       5.000000   \n",
      "max        66.000000      21.000000     144.000000       6.000000   \n",
      "\n",
      "              demand         supply            gap  \n",
      "count  190080.000000  190080.000000  190080.000000  \n",
      "mean       42.294439      42.294439       7.053609  \n",
      "std       100.144123     100.144123      38.373449  \n",
      "min         0.000000       0.000000       0.000000  \n",
      "25%         1.000000       1.000000       0.000000  \n",
      "50%         7.000000       7.000000       1.000000  \n",
      "75%        31.000000      31.000000       3.000000  \n",
      "max      1863.000000    1863.000000    1341.000000   \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Shape of train set:\", train_set.shape, \"\\n\\n\"\n",
    "print \"Column Headers:\", list(train_set.columns.values), \"\\n\\n\"\n",
    "print train_set.dtypes, \"\\n\\n\"\n",
    "print train_set.head(5), \"\\n\\n\"\n",
    "print train_set.describe(), \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMAND FORECAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split train / test data - for coarse training\n",
    "train_set.set_index('num_day', inplace=True)\n",
    "X_train = train_set[:5]\n",
    "X_test  =\n",
    "y_train =\n",
    "y_test  =\n",
    "train_set.reset_index(inplace='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPLY FORECAST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAP FORECAST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assess Feature importance\n",
    "\n",
    "# Train / Test split for original training data\n",
    "# Withold 5% from train set for testing\n",
    "predictors = ['district_id', 'num_day', 'time_slot', 'week_day', 'demand']\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "    train_set[predictors], train_set['gap'], test_size=0.05, random_state=0)\n",
    "\n",
    "print (\"Original Training Set: {}\\nTraining Set: {}\\nTesting Set(witheld): {}\" .format(train_set.shape, X_train.shape,X_test.shape))\n",
    "\n",
    "\n",
    "# Normalize features - both training & test (withheld & final)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "##VRfinal_test_transformed  = scaler.transform(test_set[predictors])\n",
    "\n",
    "print (\"Transformed training, test sets (withheld & final)\")\n",
    "\n",
    "# Scoring Metric - MSE\n",
    "print (\"Use MSE as the score function\")\n",
    "\n",
    "# Initialize the algorithm\n",
    "# Defaults to mean square error as score\n",
    "X_train1 = np.delete(X_train_transformed, [7,8], axis=1)\n",
    "\n",
    "alg = RandomForestRegressor(random_state=1, n_estimators=10000, min_samples_split=50, min_samples_leaf=1)\n",
    "clf = alg.fit(X_train1, y_train)\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_labels = ['district_id', 'num_day', 'time_slot', 'week_day', 'demand']\n",
    "for f in range(X_train1.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30, \n",
    "                             feature_labels[indices[f]], \n",
    "                             importances[indices[f]]))\n",
    "\n",
    "labels_reordered = [ feature_labels[i] for i in indices]\n",
    "    \n",
    "plt.title('Feature Importances')\n",
    "plt.bar(range(X_train1.shape[1]), \n",
    "         importances[indices],\n",
    "         color='lightblue', \n",
    "         align='center')\n",
    "plt.xticks(range(X_train1.shape[1]), labels_reordered, rotation=90)\n",
    "plt.xlim([-1, X_train1.shape[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot training data: d, s, g\n",
    "dplot = []\n",
    "splot = []\n",
    "gplot = []\n",
    "\n",
    "trange   = range(1,21*144)\n",
    "pltrange = max(trange)-min(trange)\n",
    "\n",
    "for t in trange:\n",
    "    dplot.append(d_train[(20,t)])\n",
    "    splot.append(s_train[(20,t)])\n",
    "    gplot.append(g_train[(20,t)])\n",
    "\n",
    "f, (ax1,ax2,ax3) = plt.subplots(3,1, sharex=True, figsize=(20,15))\n",
    "plt.xlim(0, pltrange)\n",
    "ax1.grid(True)\n",
    "#ax1.set_yticks(np.arange(0,160,20))\n",
    "ax1.set_xticks(np.arange(0, pltrange,144))\n",
    "    \n",
    "sns.tsplot(data=dplot, ax=ax1, value=\"demand\")\n",
    "sns.tsplot(data=splot, ax=ax2, value=\"supply\")\n",
    "sns.tsplot(data=gplot, ax=ax3, value=\"gap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_test = {}\n",
    "s_test = {}\n",
    "g_test = {}\n",
    "d_test, s_test, g_test = demand_supply(order_test)\n",
    "\n",
    "# Plot a distribution plot\n",
    "dist = range(15,30)\n",
    "time = range(21*144,31*144)\n",
    "\n",
    "dplot = []\n",
    "splot = []\n",
    "gplot = []\n",
    "for t in range(21*144,31*144):\n",
    "    dplot.append(d[(20,t)])\n",
    "    splot.append(s[(20,t)])\n",
    "    gplot.append(g[(20,t)])\n",
    "\n",
    "f, (axis1,axis2,axis3) = plt.subplots(3,1, sharex=False, figsize=(20,15))\n",
    "\n",
    "sns.distplot(dplot, hist=False, color=\"g\", kde_kws={\"shade\": True}, ax=axis1)\n",
    "sns.distplot(splot, hist=False, color=\"b\", kde_kws={\"shade\": True}, ax=axis2)\n",
    "sns.distplot(gplot, hist=False, color=\"r\", kde_kws={\"shade\": True}, ax=axis3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {'demand': dplot, 'gap': gplot}\n",
    "df = pd.DataFrame(data, columns=['demand', 'gap'])\n",
    "\n",
    "grid = sns.JointGrid(x='demand', y='gap', data=df, size=15, ratio=4)\n",
    "grid = grid.plot(sns.regplot, sns.distplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot test data\n",
    "dplot = []\n",
    "splot = []\n",
    "gplot = []\n",
    "\n",
    "trange   = range(23*144,31*144)\n",
    "pltrange = max(trange)-min(trange)\n",
    "\n",
    "for t in trange:\n",
    "    dplot.append(d[(20,t)])\n",
    "    splot.append(s[(20,t)])\n",
    "    gplot.append(g[(20,t)])\n",
    "    \n",
    "f, (axis1,axis2,axis3) = plt.subplots(3,1, sharex=True, figsize=(20,15))\n",
    "\n",
    "sns.tsplot(data=dplot, ax=axis1, value=\"demand\")\n",
    "sns.tsplot(data=splot, ax=axis2, value=\"supply\")\n",
    "sns.tsplot(data=gplot, ax=axis3, value=\"gap\")\n",
    "\n",
    "plt.xlim(0, pltrange)\n",
    "\n",
    "axis1.grid(True)\n",
    "\n",
    "#axis1.set_yticks(np.arange(0,160,20))\n",
    "axis1.set_xticks(np.arange(0, pltrange,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\")\n",
    "\n",
    "# Create a dataset with many short random walks\n",
    "rs = np.random.RandomState(4)\n",
    "pos = rs.randint(-1, 2, (20, 5)).cumsum(axis=1)\n",
    "pos -= pos[:, 0, np.newaxis]\n",
    "step = np.tile(range(5), 20)\n",
    "walk = np.repeat(range(20), 5)\n",
    "df = pd.DataFrame(np.c_[pos.flat, step, walk],\n",
    "                  columns=[\"position\", \"step\", \"walk\"])\n",
    "\n",
    "# Initialize a grid of plots with an Axes for each walk\n",
    "grid = sns.FacetGrid(df, col=\"walk\", hue=\"walk\", col_wrap=5, size=1.5)\n",
    "\n",
    "# Draw a horizontal line to show the starting point\n",
    "grid.map(plt.axhline, y=0, ls=\":\", c=\".5\")\n",
    "\n",
    "# Draw a line plot to show the trajectory of each random walk\n",
    "grid.map(plt.plot, \"step\", \"position\", marker=\"o\", ms=4)\n",
    "\n",
    "# Adjust the tick positions and labels\n",
    "grid.set(xticks=np.arange(5), yticks=[-3, 3],\n",
    "         xlim=(-.5, 4.5), ylim=(-3.5, 3.5))\n",
    "\n",
    "# Adjust the arrangement of the plots\n",
    "grid.fig.tight_layout(w_pad=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REVIEW INPUT FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Review input features (ORDER, POI, TRAFFIC, WEATHER) for training set - Part 1\n",
    "names = ['ORDER', 'POI', 'TRAFFIC', 'WEATHER']\n",
    "features = [order_train, poi_train, traffic_train, weather_train]\n",
    "\n",
    "for name, feature in zip(names, features):\n",
    "    print \"\\n\\n-----------------------\"\n",
    "    print \"{} TRAIN INFORMATION\" .format(str.upper(name))\n",
    "    print \"-----------------------\"\n",
    "    print \"Shape of training set:\", feature.shape, \"\\n\\n\"\n",
    "    print \"Column Headers:\", list(feature.columns.values), \"\\n\\n\"\n",
    "    print feature.dtypes, \"\\n\\n\"\n",
    "    print feature.head(5), \"\\n\\n\"\n",
    "    print feature.describe(), \"\\n\\n\"\n",
    "    \n",
    "# Review input features for train set - Part 2\n",
    "missing_values = []\n",
    "nonumeric_values = []\n",
    "\n",
    "names = ['ORDER', 'POI', 'TRAFFIC', 'WEATHER']\n",
    "features = [order_train, poi_train, traffic_train, weather_train]\n",
    "\n",
    "print (\"TRAIN SET INFORMATION\")\n",
    "print (\"========================\\n\")\n",
    "\n",
    "for name,feature in zip(names,features):\n",
    "    \n",
    "    print \"\\n-----------------------\"\n",
    "    print \"{} TRAIN INFORMATION\" .format(name)\n",
    "    print \"-----------------------\\n\"\n",
    "    missing_values = []\n",
    "    nonumeric_values = []\n",
    "    \n",
    "    for column in feature:\n",
    "        \n",
    "        # Find all the unique feature values\n",
    "        uniq = feature[column].unique()\n",
    "        print (\"'{}' has {} unique values\" .format(column,uniq.size))\n",
    "        if (uniq.size > 25):\n",
    "            print(\"~~Listing up to 25 unique values~~\")\n",
    "        print (uniq[0:24])\n",
    "        print (\"\\n-----------------------------------------------------------------------\\n\")\n",
    "            \n",
    "        # Find features with missing values\n",
    "        if (True in pd.isnull(uniq)):\n",
    "            s = \"{} has {} missing\" .format(column, pd.isnull(feature[column]).sum())\n",
    "            missing_values.append(s)\n",
    "    \n",
    "        # Find features with non-numeric values\n",
    "        for i in range (1, np.prod(uniq.shape)):\n",
    "            \n",
    "            if (re.match('nan', str(uniq[i]))):\n",
    "                break\n",
    "            \n",
    "            if not (re.search('(^\\d+\\.?\\d*$)|(^\\d*\\.?\\d+$)', str(uniq[i]))):\n",
    "                nonumeric_values.append(column)\n",
    "                break\n",
    "  \n",
    "    print (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "    print (\"{} Features with missing values:\\n{}\\n\\n\" .format(name, missing_values))\n",
    "    print (\"{} Features with non-numeric values:\\n{}\" .format(name, nonumeric_values))\n",
    "    print (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Review input features (ORDER, POI, TRAFFIC, WEATHER) for test set - Part 1\n",
    "names = ['ORDER', 'POI', 'TRAFFIC', 'WEATHER']\n",
    "features = [order_test, poi_test, traffic_test, weather_test]\n",
    "\n",
    "for name, feature in zip(names, features):\n",
    "    print \"\\n\\n-----------------------\"\n",
    "    print \"{} TEST INFORMATION\" .format(name)\n",
    "    print \"-----------------------\"\n",
    "    print \"Shape of test set:\", feature.shape, \"\\n\\n\"\n",
    "    print \"Column Headers:\", list(feature.columns.values), \"\\n\\n\"\n",
    "    print feature.dtypes, \"\\n\\n\"\n",
    "    print feature.head(5), \"\\n\\n\"\n",
    "    print feature.describe(), \"\\n\\n\"\n",
    "    \n",
    "# Review input features for test set - Part 2\n",
    "missing_values = []\n",
    "nonumeric_values = []\n",
    "\n",
    "names = ['ORDER', 'POI', 'TRAFFIC', 'WEATHER']\n",
    "features = [order_test, poi_test, traffic_test, weather_test]\n",
    "\n",
    "print (\"TEST SET INFORMATION\")\n",
    "print (\"========================\\n\")\n",
    "\n",
    "for name,feature in zip(names,features):\n",
    "    \n",
    "    print \"\\n-----------------------\"\n",
    "    print \"{} TEST INFORMATION\" .format(name)\n",
    "    print \"-----------------------\\n\"\n",
    "    missing_values = []\n",
    "    nonumeric_values = []\n",
    "    \n",
    "    for column in feature:\n",
    "        \n",
    "        # Find all the unique feature values\n",
    "        uniq = feature[column].unique()\n",
    "        print (\"'{}' has {} unique values\" .format(column,uniq.size))\n",
    "        if (uniq.size > 25):\n",
    "            print(\"~~Listing up to 25 unique values~~\")\n",
    "        print (uniq[0:24])\n",
    "        print (\"\\n-----------------------------------------------------------------------\\n\")\n",
    "            \n",
    "        # Find features with missing values\n",
    "        if (True in pd.isnull(uniq)):\n",
    "            s = \"{} has {} missing\" .format(column, pd.isnull(feature[column]).sum())\n",
    "            missing_values.append(s)\n",
    "    \n",
    "        # Find features with non-numeric values\n",
    "        for i in range (1, np.prod(uniq.shape)):\n",
    "            \n",
    "            if (re.match('nan', str(uniq[i]))):\n",
    "                break\n",
    "            \n",
    "            if not (re.search('(^\\d+\\.?\\d*$)|(^\\d*\\.?\\d+$)', str(uniq[i]))):\n",
    "                nonumeric_values.append(column)\n",
    "                break\n",
    "  \n",
    "    print (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "    print (\"{} Features with missing values:\\n{}\\n\\n\" .format(name, missing_values))\n",
    "    print (\"{} Features with non-numeric values:\\n{}\" .format(name, nonumeric_values))\n",
    "    print (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
