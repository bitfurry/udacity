{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOUT THIS NOTEBOOK\n",
    "## Purpose\n",
    "This notebook attempts to fit a neural network based model using TensorFlow.    \n",
    "Results of the model fitting are analyzed.\n",
    "## Input\n",
    "'data_set.pickle' generated by 'data_processing.ipynb'.\n",
    "## Output\n",
    "Results of model fitting: plots, parameters and scores.\n",
    "## Tasks Performed\n",
    "* Load library packages\n",
    "* Load pickle file\n",
    "* Split data into train & test sets\n",
    "    * Train: weeks 1 & 2, Test: week 3\n",
    "    * Perform feature scaling\n",
    "* Run the following experiments:\n",
    "    * Single layer with learning rate, regularization\n",
    "    * Add one hidden layer with batch gradient descent, dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "http://blog.altoros.com/visualizing-tensorflow-graphs-with-tensorboard.html     \n",
    "tensorboard --logdir=/tmp/tf_examples/my_model_1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD LIBRARY PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in packages from os, tensorflow, numpy, pandas, matplotlib, seaborn, sklearn & six\n"
     ]
    }
   ],
   "source": [
    "# Import the required library packages\n",
    "import os\n",
    "import re\n",
    "import timeit\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.learn as skflow\n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.learning_curve import validation_curve\n",
    "\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Settings for matplotlib, Seaborn\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Set font sizes for matplots\n",
    "plt.rcParams.update({'font.size': 15, \n",
    "                     'legend.fontsize': 'medium', \n",
    "                     'axes.titlesize': 'medium', \n",
    "                     'axes.labelsize': 'medium'})\n",
    "\n",
    "print 'Read in packages from os, tensorflow, numpy, pandas, matplotlib, seaborn, sklearn & six'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD PICKLE FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ptrain_set (199584, 55)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'data_set.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    pdata_set = save['data_set']\n",
    "    del save\n",
    "    print 'Loaded ptrain_set', pdata_set.shape\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['district_id', 'num_day', 'time_slot', 'week_day', 'demand',\n",
       "       'demand_t-1', 'demand_t-2', 'demand_t-3', 'supply', 'supply_t-1',\n",
       "       'supply_t-2', 'supply_t-3', 'gap', 'weather', 'temperature',\n",
       "       'pollution', 'poi_pc1', 'poi_pc2', 'poi_pc3', 'poi_pc4',\n",
       "       'poi_cluster', 'tj_lvl1', 'tj_lvl2', 'tj_lvl3', 'tj_lvl4', 'dist_0',\n",
       "       'dist_1', 'dist_2', 'dist_3', 'dist_4', 'dist_5', 'dist_6',\n",
       "       'numday_0', 'numday_1', 'numday_2', 'numday_3', 'numday_4', 'ts_0',\n",
       "       'ts_1', 'ts_2', 'ts_3', 'ts_4', 'ts_5', 'ts_6', 'ts_7', 'weekday_0',\n",
       "       'weekday_1', 'weekday_2', 'poi_0', 'poi_1', 'poi_2', 'wthr_0',\n",
       "       'wthr_1', 'wthr_2', 'wthr_3'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdata_set.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT DATA INTO TRAIN & TEST SETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use weeks 1 & 2 for training, week 3 for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train, X_test: (133056, 55) (66528, 55) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_days     = range(1,15)\n",
    "test_days      = range(15, 22)\n",
    "\n",
    "X_train     = pdata_set[(pdata_set['num_day'].isin(train_days))]\n",
    "X_test      = pdata_set[(pdata_set['num_day'].isin(test_days))]\n",
    "\n",
    "print \"Shape of X_train, X_test:\", X_train.shape, X_test.shape, \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate scaled features for train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "gap_predictors = ['demand_t-1', 'demand_t-2', 'demand_t-3',\n",
    "                  'supply_t-1', 'supply_t-2', 'supply_t-3',\n",
    "                  'poi_pc1', 'poi_pc2',\n",
    "                  'tj_lvl1', 'tj_lvl2', 'tj_lvl3',\n",
    "                  'ts_0', 'ts_1', 'ts_2', 'ts_3', 'ts_4', 'ts_5', 'ts_6', 'ts_7',\n",
    "                  'pollution', 'temperature',\n",
    "                  'wthr_0', 'wthr_1', 'wthr_2', 'wthr_3'\n",
    "                 ]  \n",
    "\n",
    "gX_train = []\n",
    "gy_train = []\n",
    "gX_test  = []\n",
    "gy_test  = []\n",
    "\n",
    "# Use StandardScaler to achieve zero mean and unit variance\n",
    "# Generate two scalers: input and target\n",
    "input_scaler = StandardScaler().fit(pdata_set[gap_predictors])\n",
    "target_scaler = StandardScaler().fit(pdata_set['gap'])\n",
    "\n",
    "# Scale both training & test data\n",
    "gX_train  = input_scaler.transform(X_train[gap_predictors])\n",
    "gy_train  = target_scaler.transform(X_train['gap'])\n",
    "\n",
    "gX_test = input_scaler.transform(X_test[gap_predictors])\n",
    "gy_test = target_scaler.transform(X_test['gap'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn_predictors = [0,3,1,5,2,8,4,19]\n",
    "\n",
    "train_dataset, valid_dataset, train_labels, valid_labels = train_test_split(gX_train[:, nn_predictors], gy_train, test_size=0.33)\n",
    "test_dataset = gX_test[:, nn_predictors]\n",
    "test_labels  = gy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89147, 8) (43909, 8) (89147,) (43909,)\n"
     ]
    }
   ],
   "source": [
    "print train_dataset.shape, valid_dataset.shape, train_labels.shape, valid_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENT FUNCTIONS FOR COMMON TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate Demand Supply Gap Metrics based on provided fit functions\n",
    "# Assumes expected values for gap is in namesake column\n",
    "def gap_estimate(**kwargs):\n",
    "    \"\"\"\n",
    "    Generate scores for gap.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gX_train  : array containing train features\n",
    "    gX_test   : array containing test features\n",
    "    g_fitfunc : function to use for predicting\n",
    "    \"\"\"\n",
    "    \n",
    "    gX_train     = kwargs[\"gX_train\"]\n",
    "    gX_test      = kwargs[\"gX_test\"]\n",
    "    g_fitfunc    = kwargs[\"g_fitfunc\"]\n",
    "    \n",
    "    print \"\\n\\nGAP FORECASTING\"\n",
    "    print     \"===============\"\n",
    "\n",
    "    # Generate predictions for train & test sets\n",
    "    gy_pred_train    = target_scaler.inverse_transform(g_fitfunc.predict(gX_train))\n",
    "    gy_pred_test     = target_scaler.inverse_transform(g_fitfunc.predict(gX_test))\n",
    "\n",
    "    # Extract expected train & test values\n",
    "    gy_train    = X_train['gap']\n",
    "    gy_test     = X_test['gap']\n",
    "\n",
    "    # Evaluate scores and print results\n",
    "    print_score(gy_train, gy_pred_train, gy_test, gy_pred_test) \n",
    "    return\n",
    "\n",
    "def print_score(y_train, y_pred_train, y_test, y_pred_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Present the MSE, R^2 and MAPE scores for train & test sets as a table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_train      : Array containing expected values for train set\n",
    "    y_pred_train : Array containing predicted values for train set\n",
    "    y_test       : Array containing expected values for test set\n",
    "    y_pred_test  : Array containing predicted values for test set\n",
    "    \"\"\"\n",
    "    \n",
    "    m2score_train    = metrics.mean_squared_error(y_train,    y_pred_train)\n",
    "    m2score_test     = metrics.mean_squared_error(y_test,     y_pred_test)\n",
    "\n",
    "\n",
    "    r2score_train    = metrics.r2_score(y_train,    y_pred_train)\n",
    "    r2score_test     = metrics.r2_score(y_test,     y_pred_test)\n",
    "\n",
    "    # Assumes data is for 144 time slots, 14 days (train), 7 days (test)\n",
    "    mpscore_train    = mape_score(y_train,    y_pred_train, ((144*14)-1))\n",
    "    mpscore_test     = mape_score(y_test,     y_pred_test, ((144*7)-1))\n",
    "\n",
    "\n",
    "    sets_list = [\"TRAIN\", \"TEST\"]\n",
    "\n",
    "    m2_scores = [m2score_train, m2score_test]\n",
    "    r2_scores = [r2score_train, r2score_test]\n",
    "    mp_scores = [mpscore_train, mpscore_test]\n",
    "\n",
    "\n",
    "    print '\\t\\tMEAN^2\\t\\tR2\\t\\tMAPE'\n",
    "\n",
    "    for s, m, r, mp in zip(sets_list, m2_scores, r2_scores, mp_scores):\n",
    "        print '{0:10}\\t{1:.3f}\\t\\t{2:.3f}\\t\\t{3:.3f}' .format(s, m, r, mp)\n",
    "\n",
    "\n",
    "def mape_score(exp, pred, q):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate the MAPE score value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    exp  : Array containing expected values\n",
    "    pred : Array containing predicted values\n",
    "    q    : Constant representing (number of days * number of time slots) - 1\n",
    "    \"\"\"\n",
    "    \n",
    "    mape = 0.0\n",
    "    n = 66.0\n",
    "    \n",
    "    for gap, gapX in zip(exp, pred):\n",
    "        if gap > 0:\n",
    "            mape += 1.0 * abs((gap-gapX)/gap)\n",
    "    return (mape/(n*q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Learning & Validation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv_seconds(seconds):\n",
    "    '''\n",
    "    Convert seconds to hours, minutes, seconds format.\n",
    "    '''\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return (int(h),int(m),int(s))\n",
    "\n",
    "\n",
    "def plot_validation_curve(X, y, \n",
    "                          param_name, param_range,\n",
    "                          learning_list, hidden_list, dropout_list, steps_list, batch_list, \n",
    "                          plot_title, x_label, y_label):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the validation curve for one hyperparameter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    param_name : name of the hyper parameter that is being tested\n",
    "    \n",
    "    param_range : range of the hyper parameter\n",
    "    \n",
    "    learning_list : list of learning_rates\n",
    "    \n",
    "    hidden_list : list of hidden_units\n",
    "    \n",
    "    dropout_list : list of dropout rates\n",
    "    \n",
    "    steps_list : list of steps\n",
    "    \n",
    "    batch_list : list of batch sizes\n",
    "    \n",
    "    plot_title : string\n",
    "            Title for the chart.\n",
    "            \n",
    "    x_label: label for x-axis of the plot\n",
    "    \n",
    "    y_label: lable for y-axis of the plot\n",
    "    \n",
    "    n_jobs : integer, optional\n",
    "             Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cross validation with 3 iterations to get smoother mean test and train\n",
    "    # score curves, each time with 20% data randomly selected as a validation set.\n",
    "    cv = cross_validation.ShuffleSplit(X.shape[0], n_iter=3, test_size=0.2, random_state=0)\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    train_scores, test_scores = validation_curvenn(X, \n",
    "                                                   y,\n",
    "                                                   cv,\n",
    "                                                   learning_list,\n",
    "                                                   hidden_list, \n",
    "                                                   dropout_list, \n",
    "                                                   steps_list, \n",
    "                                                   batch_list\n",
    "                                                  )\n",
    "    \n",
    "    stop = timeit.default_timer()\n",
    "    h, m, s = conv_seconds(stop - start)\n",
    "    print 'Validation Curves Runtime: {0:d}h:{1:02d}m:{2:02d}s\\n\\n' .format(h, m, s)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "        \n",
    "    # Print Scores\n",
    "    for param, train_score, test_score in zip(param_range, train_scores_mean, test_scores_mean):\n",
    "        print ('{0}: {1:.2f}\\tTrain Score: {2:.3f}\\tCV Score: {3:.3f}' .format(param_name, float(param), float(train_score), float(test_score)))\n",
    "    \n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.title(plot_title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.semilogx(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n",
    "    plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "                 color=\"g\")\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "def validation_curvenn(X, y, cv, learning_list, hidden_list, dropout_list, steps_list, batch_list):\n",
    "    \"\"\"\n",
    "    Generate train and validation scores for provided hyperparameter range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : tensor flow object that implements the \"fit\" and \"predict\" methods\n",
    "                An object of that type which is cloned for each validation.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, true/expected values\n",
    "\n",
    "    cv : cross validation function to use\n",
    "    \n",
    "    learning_list : list of learning_rates\n",
    "    \n",
    "    hidden_list : list of hidden_units\n",
    "    \n",
    "    dropout_list : list of dropout rates\n",
    "    \n",
    "    steps_list : list of steps\n",
    "    \n",
    "    batch_list : list of batch sizes\n",
    "\n",
    "    \"\"\"\n",
    "    train_scores = []\n",
    "    cv_scores = []\n",
    "\n",
    "    # Iterate over list of hyper parameters\n",
    "    import itertools\n",
    "    for learning_rate, hidden_units, dropout, steps, batch_size in itertools.product(learning_list, hidden_list, \n",
    "                                                                                     dropout_list, steps_list, batch_list):\n",
    "    \n",
    "        print ('Learning Rate: {0:}\\tHidden Units: {1:}\\tDropout: {2:}\\tSteps: {3:5d}\\tBatch Size: {4:5d}'\n",
    "               .format(learning_rate, hidden_units, float(dropout), steps, batch_size))\n",
    "        \n",
    "        # Set up optimizer, regressor\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        regressor = skflow.DNNRegressor(hidden_units=hidden_units,\n",
    "                                        optimizer=optimizer, \n",
    "                                        dropout=float(dropout)\n",
    "                                       )\n",
    "    \n",
    "        train_scores_set = []\n",
    "        cv_scores_set = []\n",
    "        \n",
    "        # Fit and generate scores\n",
    "        for train_index, cv_index in cv:\n",
    "            regressor.fit(X[train_index],\n",
    "                          y[train_index],\n",
    "                          steps=steps, \n",
    "                          batch_size=batch_size)\n",
    "    \n",
    "            # Predict and score\n",
    "            train_prediction = regressor.predict(train_dataset[train_index])\n",
    "            train_score = metrics.mean_squared_error(train_prediction, train_labels[train_index])\n",
    "            train_scores_set.append(train_score)\n",
    "        \n",
    "            cv_prediction = regressor.predict(train_dataset[cv_index])\n",
    "            cv_score = metrics.mean_squared_error(cv_prediction, train_labels[cv_index])\n",
    "            cv_scores_set.append(cv_score)\n",
    "        \n",
    "        train_scores.append(train_scores_set)\n",
    "        cv_scores.append(cv_scores_set)\n",
    "    \n",
    "    return(train_scores,cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.0001\tHidden Units: []\tDropout: 0.0\tSteps:     1\tBatch Size:  1000\n",
      "Learning Rate: 0.0001\tHidden Units: []\tDropout: 0.0\tSteps:    10\tBatch Size:  1000\n",
      "Learning Rate: 0.0001\tHidden Units: []\tDropout: 0.0\tSteps:   100\tBatch Size:  1000\n",
      "Validation Curves Runtime: 0h:00m:20s\n",
      "\n",
      "\n",
      "steps: 1.00\tTrain Score: 0.612\tCV Score: 0.610\n",
      "steps: 10.00\tTrain Score: 0.610\tCV Score: 0.608\n",
      "steps: 100.00\tTrain Score: 0.589\tCV Score: 0.586\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA44AAAIHCAYAAADQPwIwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLoDds1Hw0QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f519391b090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_validation_curve(X=train_dataset, \n",
    "                      y=train_labels, \n",
    "                      param_name='steps', \n",
    "                      param_range=[1,10,100],\n",
    "                      learning_list=[1e-4], \n",
    "                      hidden_list=[[]], \n",
    "                      dropout_list=[0], \n",
    "                      steps_list=[1,10,100], \n",
    "                      batch_list=[1000],\n",
    "                      plot_title='Neural Nets', \n",
    "                      x_label='batch size', \n",
    "                      y_label='MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Curves Runtime: 0h:00m:21s\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyper Parameters\n",
    "steps = 500001\n",
    "learning_rate = 1e-4\n",
    "batch_size = 1\n",
    "\n",
    "# Build 2 layer fully connected DNN with 10, 10 units respectively.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "regressor = skflow.DNNRegressor(hidden_units=[10, 10],\n",
    "                                optimizer=optimizer, \n",
    "                                dropout=None)\n",
    "\n",
    "plot_validation_curve(estimator=regressor, \n",
    "                      X=train_dataset, \n",
    "                      y=train_labels, \n",
    "                      param_name='dropout', \n",
    "                      param_range=[0.1,0.2,0.3,1.0], \n",
    "                      scoring='MSE', \n",
    "                      plot_title='Neural Nets', \n",
    "                      x_label='min_samples_split', \n",
    "                      y_label='MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Hidden Layers with Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.544123\n"
     ]
    }
   ],
   "source": [
    "# Hyper Parameters\n",
    "steps = 500001\n",
    "steps = 51\n",
    "learning_rate = 1e-4\n",
    "batch_size = 1\n",
    "\n",
    "# Build 2 layer fully connected DNN with 10, 10 units respectively.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "regressor = skflow.DNNRegressor(hidden_units=[10, 10], \n",
    "                                optimizer=optimizer, \n",
    "                                dropout=None)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(train_dataset[:train_subset, :], \n",
    "              train_labels[:train_subset], \n",
    "              steps=steps, \n",
    "              batch_size=batch_size)\n",
    "\n",
    "# Predict and score\n",
    "train_prediction = regressor.predict(train_dataset[:train_subset, :])\n",
    "score = metrics.mean_squared_error(train_prediction, train_labels[:train_subset])\n",
    "\n",
    "print('MSE: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.156762\n",
      "\t\tMEAN^2\t\tR2\t\tMAPE\n",
      "TRAIN     \t0.089\t\t0.898\t\t0.012\n",
      "TEST      \t0.157\t\t0.861\t\t0.307\n"
     ]
    }
   ],
   "source": [
    "# Predict and score\n",
    "test_prediction = regressor.predict(test_dataset)\n",
    "score = metrics.mean_squared_error(test_prediction, test_labels)\n",
    "\n",
    "print('MSE: {0:f}'.format(score))\n",
    "\n",
    "print_score(train_labels[:train_subset], train_prediction, test_labels, test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Hidden Layers with Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tMEAN^2\t\tR2\t\tMAPE\n",
      "TRAIN     \t0.076\t\t0.904\t\t0.014\n",
      "TEST      \t0.187\t\t0.834\t\t0.287\n"
     ]
    }
   ],
   "source": [
    "train_subset  = 8000\n",
    "\n",
    "# Hyper Parameters\n",
    "steps = 500001\n",
    "learning_rate = 1e-4\n",
    "batch_size = 1\n",
    "\n",
    "# Build 5 layer fully connected DNN with 8, 16, 16, 8, 4 units respectively.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "regressor = skflow.DNNRegressor(hidden_units=[8, 16, 16, 8, 4], optimizer=optimizer, dropout=None)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(train_dataset[:train_subset, :], train_labels[:train_subset], steps=steps, batch_size=batch_size)\n",
    "\n",
    "# Predict and score\n",
    "train_prediction = regressor.predict(train_dataset[:train_subset, :])\n",
    "test_prediction = regressor.predict(test_dataset)\n",
    "\n",
    "print_score(train_labels[:train_subset], train_prediction, test_labels, test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Hidden Layers with batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tMEAN^2\t\tR2\t\tMAPE\n",
      "TRAIN     \t0.125\t\t0.849\t\t0.273\n",
      "TEST      \t0.189\t\t0.833\t\t0.395\n"
     ]
    }
   ],
   "source": [
    "train_subset = train_dataset.shape[0]\n",
    "\n",
    "# Hyper Parameters\n",
    "steps = 50001\n",
    "learning_rate = 1e-4\n",
    "batch_size = (train_dataset.shape[0])/10\n",
    "\n",
    "\n",
    "# Build 5 layer fully connected DNN with 8, 16, 16, 8, 4 units respectively.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "regressor = skflow.DNNRegressor(hidden_units=[8, 16, 16, 8, 4], optimizer=optimizer, dropout=None)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(train_dataset[:train_subset, :], train_labels[:train_subset], steps=steps, batch_size=batch_size)\n",
    "\n",
    "# Predict and score\n",
    "train_prediction = regressor.predict(train_dataset[:train_subset, :])\n",
    "test_prediction = regressor.predict(test_dataset)\n",
    "\n",
    "print_score(train_labels[:train_subset], train_prediction, test_labels, test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlowDNNRegressor class is deprecated. Please consider using DNNRegressor as an alternative.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tMEAN^2\t\tR2\t\tMAPE\n",
      "TRAIN     \t0.125\t\t0.849\t\t0.273\n",
      "TEST      \t0.189\t\t0.833\t\t0.395\n"
     ]
    }
   ],
   "source": [
    "train_subset = train_dataset.shape[0]\n",
    "\n",
    "# Hyper Parameters\n",
    "steps = 50001\n",
    "learning_rate = 1e-4\n",
    "batch_size = (train_dataset.shape[0])/10\n",
    "\n",
    "\n",
    "# Build 5 layer fully connected DNN with 8, 16, 16, 8, 4 units respectively.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "regressor = skflow.TensorFlowDNNRegressor(hidden_units=[8, 16, 16, 8, 4], optimizer=optimizer, dropout=None)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(train_dataset[:train_subset, :], \n",
    "              train_labels[:train_subset], \n",
    "              steps=steps, batch_size=batch_size, \n",
    "              logdir='/tmp/tf_examples/my_model_2/')\n",
    "\n",
    "# Predict and score\n",
    "train_prediction = regressor.predict(train_dataset[:train_subset, :])\n",
    "test_prediction = regressor.predict(test_dataset)\n",
    "\n",
    "print_score(train_labels[:train_subset], train_prediction, test_labels, test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETS - CUSTOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer with Learning Rate, Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Subset the training data for faster turnaround.\n",
    "#train_subset  = (train_dataset.shape[0])/10\n",
    "train_subset  = 6000\n",
    "\n",
    "# Constants\n",
    "num_weights      = train_dataset.shape[1]\n",
    "\n",
    "# Hyper Parameters\n",
    "learning_rate = 1e-4\n",
    "l2loss_lambda = 1e-4\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Load the training, validation and test data into constants\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :], dtype=tf.float64)\n",
    "    tf_train_labels  = tf.constant(train_labels[:train_subset], dtype=tf.float64)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset, dtype=tf.float64)\n",
    "    tf_test_dataset  = tf.constant(test_dataset, dtype=tf.float64)\n",
    "  \n",
    "    # Initialize weight matrix using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights = tf.Variable(tf.truncated_normal([num_weights, 1], dtype=tf.float64))\n",
    "    biases  = tf.Variable(tf.zeros([1], dtype=tf.float64))\n",
    "  \n",
    "    # Regularization loss\n",
    "    beta    = tf.Variable(tf.zeros([1], dtype=tf.float64))  \n",
    "    regularization = l2loss_lambda * tf.nn.l2_loss(weights) \n",
    "\n",
    " \n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. \n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  \n",
    "    # Mean squared error + Regularization\n",
    "    loss = (tf.reduce_sum(tf.pow(tf.reshape(logits,[-1])-tf_train_labels, 2))/ train_subset) +  regularization\n",
    "    \n",
    "    \n",
    "    # Minimize cost + l2_loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = logits\n",
    "    valid_prediction = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "    test_prediction  = tf.matmul(tf_test_dataset, weights) + biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 6.469%\n",
      "Training accuracy: 6.469%\n",
      "Validation accuracy: 6.893%\n",
      "Test accuracy: 8.693%\n",
      "Loss at step 100000: 0.241%\n",
      "Training accuracy: 0.241%\n",
      "Validation accuracy: 0.129%\n",
      "Test accuracy: 0.189%\n",
      "Loss at step 200000: 0.158%\n",
      "Training accuracy: 0.157%\n",
      "Validation accuracy: 0.127%\n",
      "Test accuracy: 0.197%\n",
      "Loss at step 300000: 0.128%\n",
      "Training accuracy: 0.127%\n",
      "Validation accuracy: 0.131%\n",
      "Test accuracy: 0.203%\n",
      "Loss at step 400000: 0.114%\n",
      "Training accuracy: 0.114%\n",
      "Validation accuracy: 0.135%\n",
      "Test accuracy: 0.207%\n",
      "Loss at step 500000: 0.108%\n",
      "Training accuracy: 0.107%\n",
      "Validation accuracy: 0.137%\n",
      "Test accuracy: 0.211%\n",
      "\t\tMEAN^2\t\tR2\t\tMAPE\n",
      "TRAIN     \t0.107\t\t0.912\t\t0.023\n",
      "TEST      \t0.211\t\t0.813\t\t0.468\n"
     ]
    }
   ],
   "source": [
    "num_steps = 500001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 100000 == 0):\n",
    "            print('Loss at step %d: %.3f%%' % (step, l))\n",
    "            print('Training MSE: %.3f%%' % metrics.mean_squared_error(predictions, train_labels[:train_subset]))\n",
    "            print('Validation MSE: %.3f%%' % metrics.mean_squared_error(valid_prediction.eval(), valid_labels))\n",
    "            #print('Test MSE: %.3f%%' % metrics.mean_squared_error(test_prediction.eval(), test_labels))\n",
    "            \n",
    "            valid_predictions = valid_prediction.eval()\n",
    "            test_predictions = test_prediction.eval()\n",
    "            \n",
    "    print_score(train_labels[:train_subset], predictions.reshape(-1), test_labels, test_predictions.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1, dtype=tf.float64)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape, dtype=tf.float64)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Constants\n",
    "num_weights = train_dataset.shape[1]\n",
    "\n",
    "# Hyper Parameters\n",
    "learning_rate = 1e-5\n",
    "l2loss_lambda = 0 #1e-4\n",
    "batch_size = 10000\n",
    "num_relus  = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float64, shape=(batch_size, num_weights))\n",
    "    tf_train_labels  = tf.placeholder(tf.float64, shape=(batch_size, 1))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset  = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    weights1 = weight_variable([num_weights, num_relus])\n",
    "    biases1  = bias_variable([num_relus])\n",
    "    weights2 = weight_variable([num_relus, 1])\n",
    "    biases2  = bias_variable([1])\n",
    "        \n",
    "    # Regularization loss\n",
    "    beta    = tf.Variable(tf.zeros([1], dtype=tf.float64))  \n",
    "    l2_loss = l2loss_lambda * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))   \n",
    "\n",
    "    # Training computation.\n",
    "    to_hidden   = tf.matmul(tf_train_dataset, weights1) + biases1\n",
    "    from_hidden = tf.nn.relu(to_hidden)\n",
    "\n",
    "    # Introduce dropout before readout layer\n",
    "    keep_prob = tf.placeholder(tf.float64)\n",
    "    from_drop = tf.nn.dropout(from_hidden, keep_prob)\n",
    "                           \n",
    "    logits      = tf.matmul(from_drop, weights2) + biases2\n",
    "    \n",
    "    # Mean squared error + Regularization\n",
    "    loss = tf.reduce_sum(tf.pow(tf.reshape(logits,[-1])-tf.reshape(tf_train_labels,[-1]), 2)/ batch_size) +  l2_loss\n",
    "  \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = logits\n",
    "    valid_prediction = tf.matmul(\n",
    "                       tf.nn.relu(\n",
    "                       tf.matmul(tf_valid_dataset, weights1) + biases1), weights2) + biases2\n",
    "    test_prediction = tf.matmul(\n",
    "                      tf.nn.relu(\n",
    "                      tf.matmul(tf_test_dataset, weights1) + biases1), weights2) + biases2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 1.035%\n",
      "Training MSE: 1.035%\n",
      "Validation MSE: 1.451%\n",
      "Loss at step 20000: 0.511%\n",
      "Training MSE: 0.511%\n",
      "Validation MSE: 0.508%\n",
      "Loss at step 40000: 0.252%\n",
      "Training MSE: 0.252%\n",
      "Validation MSE: 0.351%\n",
      "Loss at step 60000: 0.187%\n",
      "Training MSE: 0.187%\n",
      "Validation MSE: 0.306%\n",
      "Loss at step 80000: 0.120%\n",
      "Training MSE: 0.120%\n",
      "Validation MSE: 0.278%\n",
      "Loss at step 100000: 0.163%\n",
      "Training MSE: 0.163%\n",
      "Validation MSE: 0.257%\n",
      "\t\tMEAN^2\t\tR2\t\tMAPE\n",
      "TRAIN     \t0.163\t\t0.808\t\t0.033\n",
      "TEST      \t0.196\t\t0.827\t\t0.322\n"
     ]
    }
   ],
   "source": [
    "# Hyper Parameter\n",
    "num_steps = 100001\n",
    "keep_probvalue = 0.5\n",
    "\n",
    "with tf.Session(graph=graph) as session2:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels.reshape(-1,1)[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob: 0.5}\n",
    "        _, l, predictions = session2.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      \n",
    "        if (step % 20000 == 0):\n",
    "            print('Loss at step %d: %.3f%%' % (step, l))\n",
    "            print('Training MSE: %.3f%%' % metrics.mean_squared_error(predictions, train_labels.reshape(-1,1)[offset:(offset + batch_size), :]))\n",
    "            print('Validation MSE: %.3f%%' % metrics.mean_squared_error(valid_prediction.eval(), valid_labels))\n",
    "            #print('Test MSE: %.3f%%' % metrics.mean_squared_error(test_prediction.eval(), test_labels))\n",
    "                        \n",
    "            valid_predictions = valid_prediction.eval()\n",
    "            test_predictions = test_prediction.eval()\n",
    "            \n",
    "    print_score(train_labels[offset:(offset + batch_size)], predictions.reshape(-1), test_labels, test_predictions.reshape(-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
