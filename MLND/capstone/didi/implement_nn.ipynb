{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOUT THIS NOTEBOOK\n",
    "## Purpose\n",
    "This notebook attempts to fit a neural network based model using TensorFlow.    \n",
    "Results of the model fitting are analyzed.\n",
    "## Input\n",
    "'data_set.pickle' generated by 'data_processing.ipynb'.\n",
    "## Output\n",
    "Results of model fitting: plots, parameters and scores.\n",
    "## Tasks Performed\n",
    "* Load library packages\n",
    "* Load pickle file\n",
    "* Split data into train & test sets\n",
    "    * Train: weeks 1 & 2, Test: week 3\n",
    "    * Perform feature scaling\n",
    "* Run the following experiments:\n",
    "    * Single layer with learning rate, regularization\n",
    "    * Add one hidden layer with batch gradient descent, dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "http://blog.altoros.com/visualizing-tensorflow-graphs-with-tensorboard.html     \n",
    "tensorboard --logdir=/tmp/tf_examples/my_model_1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD LIBRARY PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in packages from os, tensorflow, numpy, pandas, matplotlib, seaborn, sklearn & six\n"
     ]
    }
   ],
   "source": [
    "# Import the required library packages\n",
    "import os\n",
    "import re\n",
    "import timeit\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.learn as skflow\n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.learning_curve import validation_curve\n",
    "\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Settings for matplotlib, Seaborn\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Set font sizes for matplots\n",
    "plt.rcParams.update({'font.size': 15, \n",
    "                     'legend.fontsize': 'medium', \n",
    "                     'axes.titlesize': 'medium', \n",
    "                     'axes.labelsize': 'medium'})\n",
    "\n",
    "print 'Read in packages from os, tensorflow, numpy, pandas, matplotlib, seaborn, sklearn & six'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD PICKLE FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ptrain_set (199584, 55)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'data_set.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    pdata_set = save['data_set']\n",
    "    del save\n",
    "    print 'Loaded ptrain_set', pdata_set.shape\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['district_id', 'num_day', 'time_slot', 'week_day', 'demand',\n",
       "       'demand_t-1', 'demand_t-2', 'demand_t-3', 'supply', 'supply_t-1',\n",
       "       'supply_t-2', 'supply_t-3', 'gap', 'weather', 'temperature',\n",
       "       'pollution', 'poi_pc1', 'poi_pc2', 'poi_pc3', 'poi_pc4',\n",
       "       'poi_cluster', 'tj_lvl1', 'tj_lvl2', 'tj_lvl3', 'tj_lvl4', 'dist_0',\n",
       "       'dist_1', 'dist_2', 'dist_3', 'dist_4', 'dist_5', 'dist_6',\n",
       "       'numday_0', 'numday_1', 'numday_2', 'numday_3', 'numday_4', 'ts_0',\n",
       "       'ts_1', 'ts_2', 'ts_3', 'ts_4', 'ts_5', 'ts_6', 'ts_7', 'weekday_0',\n",
       "       'weekday_1', 'weekday_2', 'poi_0', 'poi_1', 'poi_2', 'wthr_0',\n",
       "       'wthr_1', 'wthr_2', 'wthr_3'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdata_set.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT DATA INTO TRAIN & TEST SETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use weeks 1 & 2 for training, week 3 for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train, X_test: (133056, 55) (66528, 55) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_days     = range(1,15)\n",
    "test_days      = range(15, 22)\n",
    "\n",
    "X_train     = pdata_set[(pdata_set['num_day'].isin(train_days))]\n",
    "X_test      = pdata_set[(pdata_set['num_day'].isin(test_days))]\n",
    "\n",
    "print \"Shape of X_train, X_test:\", X_train.shape, X_test.shape, \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate scaled features for train & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "gap_predictors = ['demand_t-1', 'demand_t-2', 'demand_t-3',\n",
    "                  'supply_t-1', 'supply_t-2', 'supply_t-3',\n",
    "                  'poi_pc1', 'poi_pc2',\n",
    "                  'tj_lvl1', 'tj_lvl2', 'tj_lvl3',\n",
    "                  'ts_0', 'ts_1', 'ts_2', 'ts_3', 'ts_4', 'ts_5', 'ts_6', 'ts_7',\n",
    "                  'pollution', 'temperature',\n",
    "                  'wthr_0', 'wthr_1', 'wthr_2', 'wthr_3'\n",
    "                 ]  \n",
    "\n",
    "gX_train = []\n",
    "gy_train = []\n",
    "gX_test  = []\n",
    "gy_test  = []\n",
    "\n",
    "# Use StandardScaler to achieve zero mean and unit variance\n",
    "# Generate two scalers: input and target\n",
    "input_scaler = StandardScaler().fit(pdata_set[gap_predictors])\n",
    "target_scaler = StandardScaler().fit(pdata_set['gap'])\n",
    "\n",
    "# Scale both training & test data\n",
    "gX_train  = input_scaler.transform(X_train[gap_predictors])\n",
    "gy_train  = target_scaler.transform(X_train['gap'])\n",
    "\n",
    "gX_test = input_scaler.transform(X_test[gap_predictors])\n",
    "gy_test = target_scaler.transform(X_test['gap'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn_predictors = [0,3,1,5,2,8,4,19]\n",
    "\n",
    "train_dataset, valid_dataset, train_labels, valid_labels = train_test_split(gX_train[:, nn_predictors], gy_train, test_size=0.33)\n",
    "test_dataset = gX_test[:, nn_predictors]\n",
    "test_labels  = gy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89147, 8) (43909, 8) (89147,) (43909,)\n"
     ]
    }
   ],
   "source": [
    "print train_dataset.shape, valid_dataset.shape, train_labels.shape, valid_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENT FUNCTIONS FOR COMMON TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate Demand Supply Gap Metrics based on provided fit functions\n",
    "# Assumes expected values for gap is in namesake column\n",
    "def gap_estimate(**kwargs):\n",
    "    \"\"\"\n",
    "    Generate scores for gap.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gX_train  : array containing train features\n",
    "    gX_test   : array containing test features\n",
    "    g_fitfunc : function to use for predicting\n",
    "    \"\"\"\n",
    "    \n",
    "    gX_train     = kwargs[\"gX_train\"]\n",
    "    gX_test      = kwargs[\"gX_test\"]\n",
    "    g_fitfunc    = kwargs[\"g_fitfunc\"]\n",
    "    \n",
    "    print \"\\n\\nGAP FORECASTING\"\n",
    "    print     \"===============\"\n",
    "\n",
    "    # Generate predictions for train & test sets\n",
    "    gy_pred_train    = target_scaler.inverse_transform(g_fitfunc.predict(gX_train))\n",
    "    gy_pred_test     = target_scaler.inverse_transform(g_fitfunc.predict(gX_test))\n",
    "\n",
    "    # Extract expected train & test values\n",
    "    gy_train    = X_train['gap']\n",
    "    gy_test     = X_test['gap']\n",
    "\n",
    "    # Evaluate scores and print results\n",
    "    print_score(gy_train, gy_pred_train, gy_test, gy_pred_test) \n",
    "    return\n",
    "\n",
    "def print_score(y_train, y_pred_train, y_test, y_pred_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Present the MSE, R^2 and MAPE scores for train & test sets as a table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_train      : Array containing expected values for train set\n",
    "    y_pred_train : Array containing predicted values for train set\n",
    "    y_test       : Array containing expected values for test set\n",
    "    y_pred_test  : Array containing predicted values for test set\n",
    "    \"\"\"\n",
    "    \n",
    "    m2score_train    = metrics.mean_squared_error(y_train,    y_pred_train)\n",
    "    m2score_test     = metrics.mean_squared_error(y_test,     y_pred_test)\n",
    "\n",
    "\n",
    "    r2score_train    = metrics.r2_score(y_train,    y_pred_train)\n",
    "    r2score_test     = metrics.r2_score(y_test,     y_pred_test)\n",
    "\n",
    "    # Assumes data is for 144 time slots, 14 days (train), 7 days (test)\n",
    "    mpscore_train    = mape_score(y_train,    y_pred_train, ((144*14)-1))\n",
    "    mpscore_test     = mape_score(y_test,     y_pred_test, ((144*7)-1))\n",
    "\n",
    "\n",
    "    sets_list = [\"TRAIN\", \"TEST\"]\n",
    "\n",
    "    m2_scores = [m2score_train, m2score_test]\n",
    "    r2_scores = [r2score_train, r2score_test]\n",
    "    mp_scores = [mpscore_train, mpscore_test]\n",
    "\n",
    "\n",
    "    print '\\t\\tMEAN^2\\t\\tR2\\t\\tMAPE'\n",
    "\n",
    "    for s, m, r, mp in zip(sets_list, m2_scores, r2_scores, mp_scores):\n",
    "        print '{0:10}\\t{1:.3f}\\t\\t{2:.3f}\\t\\t{3:.3f}' .format(s, m, r, mp)\n",
    "\n",
    "\n",
    "def mape_score(exp, pred, q):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate the MAPE score value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    exp  : Array containing expected values\n",
    "    pred : Array containing predicted values\n",
    "    q    : Constant representing (number of days * number of time slots) - 1\n",
    "    \"\"\"\n",
    "    \n",
    "    mape = 0.0\n",
    "    n = 66.0\n",
    "    \n",
    "    for gap, gapX in zip(exp, pred):\n",
    "        if gap > 0:\n",
    "            mape += 1.0 * abs((gap-gapX)/gap)\n",
    "    return (mape/(n*q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Learning & Validation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv_seconds(seconds):\n",
    "    '''\n",
    "    Convert seconds to hours, minutes, seconds format.\n",
    "    '''\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return (int(h),int(m),int(s))\n",
    "\n",
    "\n",
    "def plot_validation_curve(X, y, \n",
    "                          param_name, param_range,\n",
    "                          learning_list, hidden_list, dropout_list, steps_list, batch_list, \n",
    "                          plot_title, x_label, y_label):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the validation curve for one hyperparameter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    param_name : name of the hyper parameter that is being tested\n",
    "    \n",
    "    param_range : range of the hyper parameter\n",
    "    \n",
    "    learning_list : list of learning_rates\n",
    "    \n",
    "    hidden_list : list of hidden_units\n",
    "    \n",
    "    dropout_list : list of dropout rates\n",
    "    \n",
    "    steps_list : list of steps\n",
    "    \n",
    "    batch_list : list of batch sizes\n",
    "    \n",
    "    plot_title : string\n",
    "            Title for the chart.\n",
    "            \n",
    "    x_label: label for x-axis of the plot\n",
    "    \n",
    "    y_label: lable for y-axis of the plot\n",
    "    \n",
    "    n_jobs : integer, optional\n",
    "             Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cross validation with 3 iterations to get smoother mean test and train\n",
    "    # score curves, each time with 20% data randomly selected as a validation set.\n",
    "    cv = cross_validation.ShuffleSplit(X.shape[0], n_iter=3, test_size=0.2, random_state=0)\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    train_scores, test_scores = validation_curvenn(X, \n",
    "                                                   y,\n",
    "                                                   cv,\n",
    "                                                   learning_list,\n",
    "                                                   hidden_list, \n",
    "                                                   dropout_list, \n",
    "                                                   steps_list, \n",
    "                                                   batch_list\n",
    "                                                  )\n",
    "    \n",
    "    stop = timeit.default_timer()\n",
    "    h, m, s = conv_seconds(stop - start)\n",
    "    print 'Validation Curves Runtime: {0:d}h:{1:02d}m:{2:02d}s\\n\\n' .format(h, m, s)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "        \n",
    "    # Print Scores\n",
    "    for param, train_score, test_score in zip(param_range, train_scores_mean, test_scores_mean):\n",
    "        print ('{0}: {1:.2f}    Train Score: {2:.3f} CV Score: {3:.3f}' .format(param_name, float(param), float(train_score), float(test_score)))\n",
    "    \n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.title(plot_title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.semilogx(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n",
    "    plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "                 color=\"g\")\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "def validation_curvenn(X, y, cv, learning_list, hidden_list, dropout_list, steps_list, batch_list):\n",
    "    \"\"\"\n",
    "    Generate train and validation scores for provided hyperparameter range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : tensor flow object that implements the \"fit\" and \"predict\" methods\n",
    "                An object of that type which is cloned for each validation.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, true/expected values\n",
    "\n",
    "    cv : cross validation function to use\n",
    "    \n",
    "    learning_list : list of learning_rates\n",
    "    \n",
    "    hidden_list : list of hidden_units\n",
    "    \n",
    "    dropout_list : list of dropout rates\n",
    "    \n",
    "    steps_list : list of steps\n",
    "    \n",
    "    batch_list : list of batch sizes\n",
    "\n",
    "    \"\"\"\n",
    "    train_scores = []\n",
    "    cv_scores = []\n",
    "    train_scores_set = []\n",
    "    cv_scores_set = []\n",
    "\n",
    "    # Iterate over list of hyper parameters\n",
    "    import itertools\n",
    "    for learning_rate, hidden_units, dropout, steps, batch_size in itertools.product(learning_list, hidden_list, \n",
    "                                                                                     dropout_list, steps_list, batch_list):\n",
    "    \n",
    "        print ('Learning Rate: {0:}  Hidden Units: {1:}  Dropout: {2:}  Steps: {3:}  Batch Size: {4:}'\n",
    "               .format(learning_rate, hidden_units, float(dropout), steps, batch_size))\n",
    "        \n",
    "        # Set up optimizer, regressor\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        regressor = skflow.DNNRegressor(hidden_units=hidden_units,\n",
    "                                        optimizer=optimizer, \n",
    "                                        dropout=float(dropout)\n",
    "                                       )\n",
    "    \n",
    "        # Fit and generate scores\n",
    "        for train_index, cv_index in cv:\n",
    "            regressor.fit(X[train_index],\n",
    "                          y[train_index],\n",
    "                          steps=steps, \n",
    "                          batch_size=batch_size)\n",
    "    \n",
    "            # Predict and score\n",
    "            train_prediction = regressor.predict(train_dataset[train_index])\n",
    "            train_score = metrics.mean_squared_error(train_prediction, train_labels[train_index])\n",
    "            train_scores_set.append(train_score)\n",
    "        \n",
    "            cv_prediction = regressor.predict(train_dataset[cv_index])\n",
    "            cv_score = metrics.mean_squared_error(cv_prediction, train_labels[cv_index])\n",
    "            cv_scores_set.append(cv_score)\n",
    "        \n",
    "        train_scores.append(train_scores_set)\n",
    "        cv_scores.append(cv_scores_set)\n",
    "    \n",
    "    return(train_scores,cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.0001  Hidden Units: []  Dropout: 0.0  Steps: 500001  Batch Size: 1000\n"
     ]
    }
   ],
   "source": [
    "plot_validation_curve(X=train_dataset, \n",
    "                      y=train_labels, \n",
    "                      param_name='dropout', \n",
    "                      param_range=[1000,5000,10000,20000],\n",
    "                      learning_list=[1e-4], \n",
    "                      hidden_list=[[]], \n",
    "                      dropout_list=[0], \n",
    "                      steps_list=[500001], \n",
    "                      batch_list=[1000,5000,10000,20000],\n",
    "                      plot_title='Neural Nets', \n",
    "                      x_label='batch size', \n",
    "                      y_label='MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Curves Runtime: 0h:00m:21s\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyper Parameters\n",
    "steps = 500001\n",
    "learning_rate = 1e-4\n",
    "batch_size = 1\n",
    "\n",
    "# Build 2 layer fully connected DNN with 10, 10 units respectively.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "regressor = skflow.DNNRegressor(hidden_units=[10, 10],\n",
    "                                optimizer=optimizer, \n",
    "                                dropout=None)\n",
    "\n",
    "plot_validation_curve(estimator=regressor, \n",
    "                      X=train_dataset, \n",
    "                      y=train_labels, \n",
    "                      param_name='dropout', \n",
    "                      param_range=[0.1,0.2,0.3,1.0], \n",
    "                      scoring='MSE', \n",
    "                      plot_title='Neural Nets', \n",
    "                      x_label='min_samples_split', \n",
    "                      y_label='MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Hidden Layers with Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.544123\n"
     ]
    }
   ],
   "source": [
    "# Hyper Parameters\n",
    "steps = 500001\n",
    "steps = 51\n",
    "learning_rate = 1e-4\n",
    "batch_size = 1\n",
    "\n",
    "# Build 2 layer fully connected DNN with 10, 10 units respectively.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "regressor = skflow.DNNRegressor(hidden_units=[10, 10], \n",
    "                                optimizer=optimizer, \n",
    "                                dropout=None)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(train_dataset[:train_subset, :], \n",
    "              train_labels[:train_subset], \n",
    "              steps=steps, \n",
    "              batch_size=batch_size)\n",
    "\n",
    "# Predict and score\n",
    "train_prediction = regressor.predict(train_dataset[:train_subset, :])\n",
    "score = metrics.mean_squared_error(train_prediction, train_labels[:train_subset])\n",
    "\n",
    "print('MSE: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.156762\n",
      "\t\tMEAN^2\t\tR2\t\tMAPE\n",
      "TRAIN     \t0.089\t\t0.898\t\t0.012\n",
      "TEST      \t0.157\t\t0.861\t\t0.307\n"
     ]
    }
   ],
   "source": [
    "# Predict and score\n",
    "test_prediction = regressor.predict(test_dataset)\n",
    "score = metrics.mean_squared_error(test_prediction, test_labels)\n",
    "\n",
    "print('MSE: {0:f}'.format(score))\n",
    "\n",
    "print_score(train_labels[:train_subset], train_prediction, test_labels, test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Hidden Layers with Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tMEAN^2\t\tR2\t\tMAPE\n",
      "TRAIN     \t0.076\t\t0.904\t\t0.014\n",
      "TEST      \t0.187\t\t0.834\t\t0.287\n"
     ]
    }
   ],
   "source": [
    "train_subset  = 8000\n",
    "\n",
    "# Hyper Parameters\n",
    "steps = 500001\n",
    "learning_rate = 1e-4\n",
    "batch_size = 1\n",
    "\n",
    "# Build 5 layer fully connected DNN with 8, 16, 16, 8, 4 units respectively.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "regressor = skflow.DNNRegressor(hidden_units=[8, 16, 16, 8, 4], optimizer=optimizer, dropout=None)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(train_dataset[:train_subset, :], train_labels[:train_subset], steps=steps, batch_size=batch_size)\n",
    "\n",
    "# Predict and score\n",
    "train_prediction = regressor.predict(train_dataset[:train_subset, :])\n",
    "test_prediction = regressor.predict(test_dataset)\n",
    "\n",
    "print_score(train_labels[:train_subset], train_prediction, test_labels, test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Hidden Layers with batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tMEAN^2\t\tR2\t\tMAPE\n",
      "TRAIN     \t0.125\t\t0.849\t\t0.273\n",
      "TEST      \t0.189\t\t0.833\t\t0.395\n"
     ]
    }
   ],
   "source": [
    "train_subset = train_dataset.shape[0]\n",
    "\n",
    "# Hyper Parameters\n",
    "steps = 50001\n",
    "learning_rate = 1e-4\n",
    "batch_size = (train_dataset.shape[0])/10\n",
    "\n",
    "\n",
    "# Build 5 layer fully connected DNN with 8, 16, 16, 8, 4 units respectively.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "regressor = skflow.DNNRegressor(hidden_units=[8, 16, 16, 8, 4], optimizer=optimizer, dropout=None)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(train_dataset[:train_subset, :], train_labels[:train_subset], steps=steps, batch_size=batch_size)\n",
    "\n",
    "# Predict and score\n",
    "train_prediction = regressor.predict(train_dataset[:train_subset, :])\n",
    "test_prediction = regressor.predict(test_dataset)\n",
    "\n",
    "print_score(train_labels[:train_subset], train_prediction, test_labels, test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlowDNNRegressor class is deprecated. Please consider using DNNRegressor as an alternative.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tMEAN^2\t\tR2\t\tMAPE\n",
      "TRAIN     \t0.125\t\t0.849\t\t0.273\n",
      "TEST      \t0.189\t\t0.833\t\t0.395\n"
     ]
    }
   ],
   "source": [
    "train_subset = train_dataset.shape[0]\n",
    "\n",
    "# Hyper Parameters\n",
    "steps = 50001\n",
    "learning_rate = 1e-4\n",
    "batch_size = (train_dataset.shape[0])/10\n",
    "\n",
    "\n",
    "# Build 5 layer fully connected DNN with 8, 16, 16, 8, 4 units respectively.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "regressor = skflow.TensorFlowDNNRegressor(hidden_units=[8, 16, 16, 8, 4], optimizer=optimizer, dropout=None)\n",
    "\n",
    "# Fit\n",
    "regressor.fit(train_dataset[:train_subset, :], \n",
    "              train_labels[:train_subset], \n",
    "              steps=steps, batch_size=batch_size, \n",
    "              logdir='/tmp/tf_examples/my_model_2/')\n",
    "\n",
    "# Predict and score\n",
    "train_prediction = regressor.predict(train_dataset[:train_subset, :])\n",
    "test_prediction = regressor.predict(test_dataset)\n",
    "\n",
    "print_score(train_labels[:train_subset], train_prediction, test_labels, test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETS - CUSTOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer with Learning Rate, Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Subset the training data for faster turnaround.\n",
    "#train_subset  = (train_dataset.shape[0])/10\n",
    "train_subset  = 6000\n",
    "\n",
    "# Constants\n",
    "num_weights      = train_dataset.shape[1]\n",
    "\n",
    "# Hyper Parameters\n",
    "learning_rate = 1e-4\n",
    "l2loss_lambda = 1e-4\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Load the training, validation and test data into constants\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :], dtype=tf.float64)\n",
    "    tf_train_labels  = tf.constant(train_labels[:train_subset], dtype=tf.float64)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset, dtype=tf.float64)\n",
    "    tf_test_dataset  = tf.constant(test_dataset, dtype=tf.float64)\n",
    "  \n",
    "    # Initialize weight matrix using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights = tf.Variable(tf.truncated_normal([num_weights, 1], dtype=tf.float64))\n",
    "    biases  = tf.Variable(tf.zeros([1], dtype=tf.float64))\n",
    "  \n",
    "    # Regularization loss\n",
    "    beta    = tf.Variable(tf.zeros([1], dtype=tf.float64))  \n",
    "    regularization = l2loss_lambda * tf.nn.l2_loss(weights) \n",
    "\n",
    " \n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. \n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  \n",
    "    # Mean squared error + Regularization\n",
    "    loss = (tf.reduce_sum(tf.pow(tf.reshape(logits,[-1])-tf_train_labels, 2))/ train_subset) +  regularization\n",
    "    \n",
    "    \n",
    "    # Minimize cost + l2_loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = logits\n",
    "    valid_prediction = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "    test_prediction  = tf.matmul(tf_test_dataset, weights) + biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 6.469%\n",
      "Training accuracy: 6.469%\n",
      "Validation accuracy: 6.893%\n",
      "Test accuracy: 8.693%\n",
      "Loss at step 100000: 0.241%\n",
      "Training accuracy: 0.241%\n",
      "Validation accuracy: 0.129%\n",
      "Test accuracy: 0.189%\n",
      "Loss at step 200000: 0.158%\n",
      "Training accuracy: 0.157%\n",
      "Validation accuracy: 0.127%\n",
      "Test accuracy: 0.197%\n",
      "Loss at step 300000: 0.128%\n",
      "Training accuracy: 0.127%\n",
      "Validation accuracy: 0.131%\n",
      "Test accuracy: 0.203%\n",
      "Loss at step 400000: 0.114%\n",
      "Training accuracy: 0.114%\n",
      "Validation accuracy: 0.135%\n",
      "Test accuracy: 0.207%\n",
      "Loss at step 500000: 0.108%\n",
      "Training accuracy: 0.107%\n",
      "Validation accuracy: 0.137%\n",
      "Test accuracy: 0.211%\n",
      "\t\tMEAN^2\t\tR2\t\tMAPE\n",
      "TRAIN     \t0.107\t\t0.912\t\t0.023\n",
      "TEST      \t0.211\t\t0.813\t\t0.468\n"
     ]
    }
   ],
   "source": [
    "num_steps = 500001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 100000 == 0):\n",
    "            print('Loss at step %d: %.3f%%' % (step, l))\n",
    "            print('Training MSE: %.3f%%' % metrics.mean_squared_error(predictions, train_labels[:train_subset]))\n",
    "            print('Validation MSE: %.3f%%' % metrics.mean_squared_error(valid_prediction.eval(), valid_labels))\n",
    "            #print('Test MSE: %.3f%%' % metrics.mean_squared_error(test_prediction.eval(), test_labels))\n",
    "            \n",
    "            valid_predictions = valid_prediction.eval()\n",
    "            test_predictions = test_prediction.eval()\n",
    "            \n",
    "    print_score(train_labels[:train_subset], predictions.reshape(-1), test_labels, test_predictions.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1, dtype=tf.float64)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape, dtype=tf.float64)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Constants\n",
    "num_weights = train_dataset.shape[1]\n",
    "\n",
    "# Hyper Parameters\n",
    "learning_rate = 1e-5\n",
    "l2loss_lambda = 0 #1e-4\n",
    "batch_size = 10000\n",
    "num_relus  = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float64, shape=(batch_size, num_weights))\n",
    "    tf_train_labels  = tf.placeholder(tf.float64, shape=(batch_size, 1))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset  = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    weights1 = weight_variable([num_weights, num_relus])\n",
    "    biases1  = bias_variable([num_relus])\n",
    "    weights2 = weight_variable([num_relus, 1])\n",
    "    biases2  = bias_variable([1])\n",
    "        \n",
    "    # Regularization loss\n",
    "    beta    = tf.Variable(tf.zeros([1], dtype=tf.float64))  \n",
    "    l2_loss = l2loss_lambda * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))   \n",
    "\n",
    "    # Training computation.\n",
    "    to_hidden   = tf.matmul(tf_train_dataset, weights1) + biases1\n",
    "    from_hidden = tf.nn.relu(to_hidden)\n",
    "\n",
    "    # Introduce dropout before readout layer\n",
    "    keep_prob = tf.placeholder(tf.float64)\n",
    "    from_drop = tf.nn.dropout(from_hidden, keep_prob)\n",
    "                           \n",
    "    logits      = tf.matmul(from_drop, weights2) + biases2\n",
    "    \n",
    "    # Mean squared error + Regularization\n",
    "    loss = tf.reduce_sum(tf.pow(tf.reshape(logits,[-1])-tf.reshape(tf_train_labels,[-1]), 2)/ batch_size) +  l2_loss\n",
    "  \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = logits\n",
    "    valid_prediction = tf.matmul(\n",
    "                       tf.nn.relu(\n",
    "                       tf.matmul(tf_valid_dataset, weights1) + biases1), weights2) + biases2\n",
    "    test_prediction = tf.matmul(\n",
    "                      tf.nn.relu(\n",
    "                      tf.matmul(tf_test_dataset, weights1) + biases1), weights2) + biases2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 1.035%\n",
      "Training MSE: 1.035%\n",
      "Validation MSE: 1.451%\n",
      "Loss at step 20000: 0.511%\n",
      "Training MSE: 0.511%\n",
      "Validation MSE: 0.508%\n",
      "Loss at step 40000: 0.252%\n",
      "Training MSE: 0.252%\n",
      "Validation MSE: 0.351%\n",
      "Loss at step 60000: 0.187%\n",
      "Training MSE: 0.187%\n",
      "Validation MSE: 0.306%\n",
      "Loss at step 80000: 0.120%\n",
      "Training MSE: 0.120%\n",
      "Validation MSE: 0.278%\n",
      "Loss at step 100000: 0.163%\n",
      "Training MSE: 0.163%\n",
      "Validation MSE: 0.257%\n",
      "\t\tMEAN^2\t\tR2\t\tMAPE\n",
      "TRAIN     \t0.163\t\t0.808\t\t0.033\n",
      "TEST      \t0.196\t\t0.827\t\t0.322\n"
     ]
    }
   ],
   "source": [
    "# Hyper Parameter\n",
    "num_steps = 100001\n",
    "keep_probvalue = 0.5\n",
    "\n",
    "with tf.Session(graph=graph) as session2:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels.reshape(-1,1)[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob: 0.5}\n",
    "        _, l, predictions = session2.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      \n",
    "        if (step % 20000 == 0):\n",
    "            print('Loss at step %d: %.3f%%' % (step, l))\n",
    "            print('Training MSE: %.3f%%' % metrics.mean_squared_error(predictions, train_labels.reshape(-1,1)[offset:(offset + batch_size), :]))\n",
    "            print('Validation MSE: %.3f%%' % metrics.mean_squared_error(valid_prediction.eval(), valid_labels))\n",
    "            #print('Test MSE: %.3f%%' % metrics.mean_squared_error(test_prediction.eval(), test_labels))\n",
    "                        \n",
    "            valid_predictions = valid_prediction.eval()\n",
    "            test_predictions = test_prediction.eval()\n",
    "            \n",
    "    print_score(train_labels[offset:(offset + batch_size)], predictions.reshape(-1), test_labels, test_predictions.reshape(-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
